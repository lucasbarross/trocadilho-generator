{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "##  Trocadilho Generator \n",
        "\n",
        "Notebook modified from the original by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read this [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use the original notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_"
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6ceff6-3297-4977-e101-31765b0bb4c7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  7 21:02:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cd3288-54ca-474d-ffc0-27b6b71dc22f"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 471Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 966kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 573Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:11, 6.97Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 519Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 1.18Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 1.15Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca17182-bde2-4f51-fcdb-d736f5ce19bf"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Downloading a Text File to be Trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/lucasbarross/trocadilho-generator/main/training/jokes.txt\"\n",
        "data = requests.get(url)\n",
        "file_name = \"jokes.txt\"\n",
        "\n",
        "with open(file_name, 'w') as f:\n",
        "  f.write(data.text)\n",
        "\n",
        "gpt2.copy_file_from_gdrive(\"../../\"+file_name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "The next cell will start the actual finetuning/training of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800b888d-d71c-4c62-8dd7-bbc8fa3e77b3"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              learning_rate=1e-5,\n",
        "              model_name='124M',\n",
        "              steps=1500,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 253358 tokens\n",
            "Training...\n",
            "[10 | 45.54] loss=4.12 avg=4.12\n",
            "[20 | 87.10] loss=3.70 avg=3.91\n",
            "[30 | 128.70] loss=3.81 avg=3.88\n",
            "[40 | 170.27] loss=3.94 avg=3.89\n",
            "[50 | 211.83] loss=3.89 avg=3.89\n",
            "[60 | 253.36] loss=3.57 avg=3.84\n",
            "[70 | 294.73] loss=3.59 avg=3.80\n",
            "[80 | 336.49] loss=3.49 avg=3.76\n",
            "[90 | 378.20] loss=3.72 avg=3.75\n",
            "[100 | 420.13] loss=3.47 avg=3.73\n",
            "[110 | 462.18] loss=3.50 avg=3.70\n",
            "[120 | 504.25] loss=3.38 avg=3.67\n",
            "[130 | 546.32] loss=3.44 avg=3.66\n",
            "[140 | 588.28] loss=2.97 avg=3.60\n",
            "[150 | 630.29] loss=3.35 avg=3.59\n",
            "[160 | 672.28] loss=3.24 avg=3.56\n",
            "[170 | 714.28] loss=3.37 avg=3.55\n",
            "[180 | 756.24] loss=3.31 avg=3.54\n",
            "[190 | 798.18] loss=3.29 avg=3.52\n",
            "[200 | 840.11] loss=3.23 avg=3.51\n",
            "======== SAMPLE 1 ========\n",
            "s are better because we eat and do our job, better because we have more time. And so for me it was as if people don't care at all about, and not just for, a football player's time. So when he got his contract, it was just for him. It's a bit strange, because it makes you do better.\"\n",
            "\n",
            "In other words, Neymar's time is better for Barcelona than for L.S. Neymar's time is better for L.S. Neymar's time is better for L.S.\n",
            "\n",
            "Neymar was a starlet for Barcelona and L.S. Neymar is a starlet for L.S. It appears that there's also some debate...\n",
            "\n",
            "\"You are better at a footballer's time, O Jesus, O Jesus. Because L.S. Neymar is not a starlet at football club. O Jesus. Jesus ... Jesus.\"\n",
            "\n",
            "This is L.S.'s first top-end move. L.S. Neymar is just a starlet at a football club. O Jesus, and L.S.\" O Jesus, O Jesus, O Jesus ... Jesus ... Jesus ... Jesus ... Jesus ...!\"\n",
            "\n",
            "Pokalou! Here's what happens...\n",
            "\n",
            "\"L.S. Neymar is a starlet at a football club in Lidrome. ... Jesus! ... Jesus! ... Jesus!\" O Jesus, O Jesus, O Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ...!\" ... O Jesus, O Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ...!\" O Jesus, O Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ...\n",
            "\n",
            "[210 | 901.35] loss=3.08 avg=3.48\n",
            "[220 | 943.34] loss=3.15 avg=3.47\n",
            "[230 | 985.30] loss=3.17 avg=3.45\n",
            "[240 | 1027.21] loss=3.00 avg=3.43\n",
            "[250 | 1069.17] loss=3.07 avg=3.41\n",
            "[260 | 1111.13] loss=3.09 avg=3.40\n",
            "[270 | 1153.10] loss=3.02 avg=3.38\n",
            "[280 | 1195.06] loss=3.01 avg=3.37\n",
            "[290 | 1237.04] loss=3.00 avg=3.35\n",
            "[300 | 1279.01] loss=3.10 avg=3.34\n",
            "[310 | 1320.99] loss=2.90 avg=3.33\n",
            "[320 | 1363.00] loss=3.01 avg=3.32\n",
            "[330 | 1404.99] loss=2.67 avg=3.29\n",
            "[340 | 1446.98] loss=3.04 avg=3.28\n",
            "[350 | 1488.93] loss=3.03 avg=3.28\n",
            "[360 | 1530.86] loss=2.99 avg=3.27\n",
            "[370 | 1572.78] loss=3.15 avg=3.26\n",
            "[380 | 1614.74] loss=3.00 avg=3.25\n",
            "[390 | 1656.70] loss=3.04 avg=3.25\n",
            "[400 | 1698.58] loss=2.91 avg=3.24\n",
            "======== SAMPLE 1 ========\n",
            "|\n",
            "\n",
            "Jovem: \"Meu vai\". N√£o muda. <|endofjoke|>\n",
            "\n",
            "O que disse pra faz que j√° praisente? √â sola mai-vi! <|endofjoke|>\n",
            "\n",
            "Qual est√° sua estafa dos japonos? N√∫mera de Aproxem! <|endofjoke|>\n",
            "\n",
            "Porque os asquereses de asis que n√£o eu t√° uma estafa... √â carvalho! <|endofjoke|>\n",
            "\n",
            "Quem especialmente da terra√ßa... Para seus eleu... <|endofjoke|>\n",
            "\n",
            "Qual a luzca est√° quando dessa a passa do √ïnibus? Tiro-A-Jove <|endofjoke|>\n",
            "\n",
            "Qual a segunda que diz e a ruta pra se calto? O Vaz-Vaz <|endofjoke|>\n",
            "\n",
            "O que √© mata gosta com um sistema? Sainte <|endofjoke|>\n",
            "\n",
            "Empoca que tem c√≥digo de comer as cada que s√≥ que tem as cada? <|endofjoke|>\n",
            "\n",
            "Dixiela que √© que se o carro das seus escalar√°ndico? Eu vinga eu gostava. <|endofjoke|>\n",
            "\n",
            "Por que os pessoas com o nome do carro? Por que Ele aquele aquele que ele um carro de carro. <|endofjoke|>\n",
            "\n",
            "Vive que estor quando uma s√≥ perguntiva em comunicado? Voc√™ n√£o g√™das n√£o entrou eles, at√© a√ßou ela √© cada um cidade a√ßuc√£o <|endofjoke|>\n",
            "\n",
            "Qual o jogo e que o N-star que a joveme o n√£o o que acontece? A vinta. <|endofjoke|>\n",
            "\n",
            "Compulsou o meixico um sessores dif√≠cil? Bifid√£o que a√ß√∫car. <|endofjoke|>\n",
            "\n",
            "Qual jornal de tomates mais de cantatos do mundo? No, a √ölamas como o v√≠rus <|endofjoke|>\n",
            "\n",
            "Como se churrle √© a cidade come d√° seria? Quatro no m√£e de uma uma eu, vc? Ela do um, uma eu eu, uma eu eeu. <|endofjoke|>\n",
            "\n",
            "Qual o corretor que eu eu seus faltas? Estado, eu seus nascimento. <|endofjoke|>\n",
            "\n",
            "Quem voc√™ vai que uma nome para at√© ainda quando uma rios de venceu? Um cara. <|endofjoke|>\n",
            "\n",
            "Voc√™ el√™s uma seu estilo, sempre estove a seu ou ou√ßo uma pega o dia ele n√£o √©? O seus-e-meu. <|endofjoke|>\n",
            "\n",
            "Todos mais cama as menos e suam m√£e do Marou e meu oucem. S√£o ocuni <|endofjoke|>\n",
            "\n",
            "Voc√™ sabe esse perto soga do mundo... O jogo que o √∫nico. <|endofjoke|>\n",
            "\n",
            "Fui aquele gosta. Qual diferen√ßa n√£o atingido? Ele queria, avelia. <|endofjoke|>\n",
            "\n",
            "Borra sabe a vinaiga, porque foi fazendo. Porque ele √© o √© n√£o d√° fica a melhor! <|endofjoke|>\n",
            "\n",
            "Se o crava o que o v√™ n√£o se vi? Que pada? <|endofjoke|>\n",
            "\n",
            "Quem sempre fazer ou uma teia? Quando se umas n√£o seus n√¥les <|endofjoke|>\n",
            "\n",
            "Por que o jogo vender um perto? Qual o jogo? Para est√£o de ap√≥s <|endofjoke|>\n",
            "\n",
            "Qual mas m√°cesses que esperanhas em uma\n",
            "\n",
            "[410 | 1758.52] loss=2.78 avg=3.22\n",
            "[420 | 1800.39] loss=2.92 avg=3.22\n",
            "[430 | 1842.30] loss=2.93 avg=3.21\n",
            "[440 | 1884.20] loss=2.89 avg=3.20\n",
            "[450 | 1926.13] loss=2.36 avg=3.18\n",
            "[460 | 1968.05] loss=2.90 avg=3.17\n",
            "[470 | 2009.98] loss=2.81 avg=3.16\n",
            "[480 | 2051.95] loss=2.91 avg=3.15\n",
            "[490 | 2093.86] loss=2.85 avg=3.14\n",
            "[500 | 2135.84] loss=2.72 avg=3.13\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 2180.93] loss=2.87 avg=3.13\n",
            "[520 | 2222.83] loss=2.73 avg=3.12\n",
            "[530 | 2264.61] loss=2.74 avg=3.11\n",
            "[540 | 2306.36] loss=2.68 avg=3.10\n",
            "[550 | 2348.06] loss=2.84 avg=3.09\n",
            "[560 | 2389.79] loss=2.65 avg=3.08\n",
            "[570 | 2431.47] loss=2.80 avg=3.07\n",
            "[580 | 2473.17] loss=2.78 avg=3.07\n",
            "[590 | 2514.89] loss=2.73 avg=3.06\n",
            "[600 | 2556.65] loss=2.80 avg=3.05\n",
            "======== SAMPLE 1 ========\n",
            " engendido, o nome de um amigo para m√≠dio? O, na √∫ltima. <|endofjoke|>\n",
            "O que o ouro mais tio do seu m√£e? O, um √© o nome √© o que m√£e de tiver se engenhei? Engenhei de t√£o. <|endofjoke|>\n",
            "A cidade ficou a chamada O que vai n√£o ent√£o do chamada. <|endofjoke|>\n",
            "Fui, sabia da chorar com uma carro de um sistema que ele nanch√≥ a m√£e, ach√° de vai de vem sei o a√ßutal. Qual o nome do filme? A c√©u de biscofo. <|endofjoke|>\n",
            "Um cachorro estraram um a√∫do... Tiveles! <|endofjoke|>\n",
            "O n√∫mero c√¢mara de √°gua que voc√™ gostava? Qual o ministro do casa que se a casa de casa ? <|endofjoke|>\n",
            "Qual a m√∫sica mais ao doen√ßa? A √∫ltimo O C√©sar! <|endofjoke|>\n",
            "Por que o s√≠ndico mais tanto o p√£o de √°rea? Por que ele √© um eu de ocasiado. <|endofjoke|>\n",
            "Qual o t√£o perto a corre? A lite. <|endofjoke|>\n",
            "C√©u v√¥ faz o p√£o... √âu faz o p√£o de fazer. <|endofjoke|>\n",
            "Euvir se acidente √© ciudad dela√ßo, que cinco as f√≥rmas os piscivos muitas comunismo poru, √© a sua t√™m ou ciudade. √â o cinco de fil√≥sofo... <|endofjoke|>\n",
            "Quem √© o cinco as a pessoas? O galinh√£o <|endofjoke|>\n",
            "Voc√™ √© o uma otorrido do meu doje uma otorrido do usado. Qual diferen√ßa a l√≠ngua? Tinto. <|endofjoke|>\n",
            "O que aconteceu quando n√£o era p√°? ... eu acho, o √∫ltimo um aula e ele fazer: a tinto... <|endofjoke|>\n",
            "Por que a casa que sei o um fazer chamado? Porque ele vai √© a casa, n√£o ele tem a casa de f√≥mente. <|endofjoke|>\n",
            "Qual o nome do f√≥rmulas que quando n√£o pode sai quando n√£o seguro? Porque ele s√£o foi ao jama√ßonel. <|endofjoke|>\n",
            "Qual o ve√≠culo que n√£o vai com o que uma minha? O p√£o de ficar-choco. <|endofjoke|>\n",
            "Qual o p√£o de corre o m√∫sico mais su√°rio? O Ticuyinho <|endofjoke|>\n",
            "Qual o nome daquela ao pessoa que ele frio da p√£o do mundo? O a B√£es-m√≠-t√£o <|endofjoke|>\n",
            "J√° contar o cara do p√∫blico que ele um poca do minora no a√±√°rio. N√£o j√° faz, t√° sempre querer muito vam a p√£o de p√™ <|endofjoke|>\n",
            "O que o fala foi fazer? Foi, fei fei <|endofjoke|>\n",
            "O que √© um japona gosta n√£o crija foi? eu gosta, √© um japona gosta. <|endofjoke|>\n",
            "Cera sempre um corre o outro, seu fala de t√™m na cerveja. Aquele quem n√£o √© o outro nel e n√£o falta, n√£o √© o crija? <|endofjoke|>\n",
            "O que aconteceu para o crian√ßa do cara que est√° em bate de casa? Porque ela estava a semel\n",
            "\n",
            "[610 | 2616.47] loss=2.69 avg=3.05\n",
            "[620 | 2658.17] loss=2.70 avg=3.04\n",
            "[630 | 2699.94] loss=2.72 avg=3.03\n",
            "[640 | 2741.69] loss=2.69 avg=3.03\n",
            "[650 | 2783.43] loss=2.53 avg=3.02\n",
            "[660 | 2825.16] loss=2.67 avg=3.01\n",
            "[670 | 2866.83] loss=2.54 avg=3.00\n",
            "[680 | 2908.58] loss=2.58 avg=2.99\n",
            "[690 | 2950.28] loss=2.68 avg=2.98\n",
            "[700 | 2991.95] loss=2.52 avg=2.97\n",
            "[710 | 3033.66] loss=2.70 avg=2.97\n",
            "[720 | 3075.34] loss=2.65 avg=2.96\n",
            "[730 | 3117.06] loss=2.63 avg=2.96\n",
            "[740 | 3158.76] loss=2.40 avg=2.95\n",
            "[750 | 3200.48] loss=2.46 avg=2.94\n",
            "[760 | 3242.20] loss=2.52 avg=2.93\n",
            "[770 | 3283.93] loss=2.65 avg=2.92\n",
            "[780 | 3325.70] loss=2.38 avg=2.91\n",
            "[790 | 3367.48] loss=2.47 avg=2.91\n",
            "[800 | 3409.25] loss=2.44 avg=2.90\n",
            "======== SAMPLE 1 ========\n",
            "jos a atirante? O que vender das √©cremotes. <|endofjoke|> \n",
            "Um queijor de cagemante anos do f√©rias pra outro f√°brica... √â o filha. <|endofjoke|> \n",
            "Por que a mulher de casa da vila na cama? Nada. <|endofjoke|> \n",
            "Por que a mulher de pico e ani a o ponto-l√° pra embarada? P E A P R <|endofjoke|> \n",
            "Voc√™s sabem quatro jogaam ao vender? O homem vampir, que uma √©vidigo. <|endofjoke|> \n",
            "Aos 14 meio sabenho pra se de cabe√ßao... Quem √© da sabeste de nacha? <|endofjoke|> \n",
            "Porque a roupa n√£o mais de v√≠deo ser peixe? Porque eles a pessoa da M√£e. <|endofjoke|> \n",
            "Algu√©m a √°rea de fofava de chega de crian√ßa o outro foIeu a m√£e de vida? FoIeu, a-m√£o o daquele-m√£e. <|endofjoke|> \n",
            "Qual √© a maior f√≥rem do tempo do tempo do tempo da √°gua?  O tempo do tempo do tempo do tempo do tempo... <|endofjoke|> \n",
            "Por que o cumb√≠lia √© fica no √∫ltima m√∫sica? Porque √© est√£o m√∫ses <|endofjoke|> \n",
            "Qual √© a piada da √°gua? A piada do filho. <|endofjoke|> \n",
            "Minha temo um pr√°tica oqueemia do mundo. Um dia de garrulhas. O nacha da √∫ltimo de jacar√£o. <|endofjoke|> \n",
            "Qual √© a bola de sa√∫de o deve A trifidro para o da quita dos escola√ß√µes? A trifidro de sa√∫de o comida: \"Para no dia de sa√∫ce\". <|endofjoke|> \n",
            "Quais vegas chamam um m√≥rdica o n√∫mero preferido daqui? A trifidro da estrela√ß√£o <|endofjoke|> \n",
            "Qual empresa do mundo que os n√≥s seguras? √â um nena, no uma n√∫mero <|endofjoke|> \n",
            "Qual entre dos n√≥s tem as pessoas e pessoas n√£o acusa? A m√£e, eles n√£o tem <|endofjoke|> \n",
            "Por que ano pra mais triste para qu√™? Porque ela est√£o dizer a cabe√ßa <|endofjoke|> \n",
            "Qual √© a foi que triste dos chamados? Foi a√≠. <|endofjoke|> \n",
            "Por que o b√°bex fosse pode pro nome n√£o dizem? Porque ele manda n√£o tem. <|endofjoke|> \n",
            "A jornada dar o √≥ual de trabalha que o que a f√≥rmula sozinho. Qual o nome do filme? A B√°bex triste <|endofjoke|> \n",
            "Por que no b√°bex fica de triste? Porque n√£o falam eles fez amas. <|endofjoke|> \n",
            "O que aconteceu quando elefantes cerveja sambuches foi encontro? Ele ficou porque estava pia de parecs, n√£o sei de um ficar de pisa. <|endofjoke|> \n",
            "O que √© um faz de ser um aplicada de √Ücade? A d√≥gada e seu comum. <|endofjoke|> \n",
            "Qual √© o estilo que toma um pessoa amazoninha? A m√£e pessoa, eles nunca s√≥ pessas de sartor. <|endofjoke|> \n",
            "Qual o carro que s√≥ faz t√¥? √â o c√≥logo\n",
            "\n",
            "[810 | 3469.23] loss=2.49 avg=2.89\n",
            "[820 | 3511.00] loss=2.50 avg=2.88\n",
            "[830 | 3552.78] loss=2.41 avg=2.87\n",
            "[840 | 3594.56] loss=2.44 avg=2.87\n",
            "[850 | 3636.30] loss=2.25 avg=2.86\n",
            "[860 | 3678.04] loss=2.56 avg=2.85\n",
            "[870 | 3719.81] loss=2.32 avg=2.84\n",
            "[880 | 3761.58] loss=2.30 avg=2.83\n",
            "[890 | 3803.33] loss=2.48 avg=2.83\n",
            "[900 | 3845.07] loss=2.53 avg=2.82\n",
            "[910 | 3886.80] loss=2.65 avg=2.82\n",
            "[920 | 3928.50] loss=2.41 avg=2.81\n",
            "[930 | 3970.20] loss=2.52 avg=2.81\n",
            "[940 | 4011.95] loss=2.38 avg=2.80\n",
            "[950 | 4053.64] loss=2.48 avg=2.80\n",
            "[960 | 4095.41] loss=2.40 avg=2.79\n",
            "[970 | 4137.23] loss=1.55 avg=2.77\n",
            "[980 | 4179.06] loss=2.37 avg=2.76\n",
            "[990 | 4220.94] loss=2.32 avg=2.76\n",
            "[1000 | 4262.82] loss=2.26 avg=2.75\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "Cherie! <|endofjoke|>\n",
            "O que aconteceu com um amigo conhecido? Um jovial! <|endofjoke|>\n",
            "O que acontece convidiu se um cachorro com fazer? Cacha√ßa-fra-lada. <|endofjoke|>\n",
            "Por que as amigas n√£o foi ganhar de voc√™ quando ele √© voc√™ do que no m√≠quilo? Porque ele deixa o voc√™ n√£o me ainda. <|endofjoke|>\n",
            "Por que a √°gua vai no meio do pav√™? Pq √© um ar√™. <|endofjoke|>\n",
            "Paiso para o padre dois japon√™s? <|endofjoke|>\n",
            "O que √© um jogado? √â um russa. <|endofjoke|>\n",
            "O que √© um jogado? √â um rofa. <|endofjoke|>\n",
            "o que √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© √© no puma o nome √©? √â nome. <|endofjoke|>\n",
            "Estaram um maconhece e como pessoas? Queria pessoas n√£o conhece que, o Deus conhece em mais? <|endofjoke|>\n",
            "Qual √© o animal mais que est√° sempre sem mam√£os? Porque √© a pessoa pra sem cumpr√£o. <|endofjoke|>\n",
            "Os japoneses de f√≠sica e n√£o falam. <|endofjoke|>\n",
            "Por que o vampiro as pessoas, peixes? Porque o vampros na √°gua. <|endofjoke|>\n",
            "O que √© uma sambut√† no s√©rie na igreja? √â o C√©sar as m√£os <|endofjoke|>\n",
            "Vcs gosta de fofilo que √© o de um p√©s. Para pedia ele dizer um e-verde. <|endofjoke|>\n",
            "Qual o homem que gosta de fofo? O fofo-queleo. <|endofjoke|>\n",
            "O que √© o que √© n√£o se chama pra ser moto? \"A\" \"l√°-cidade\" \"S√£o-l√°-la\" \"la\". <|endofjoke|>\n",
            "Pq a liga v√°rias t√° cagarada? Pq u√≠-ra√≠a <|endofjoke|>\n",
            "Qual o nome da cria que todo mundo visto? Urdin T√≥usko. <|endofjoke|>\n",
            "Como se eu nome a cerveja da tarda? Ele m√° cerveja da tarda. <|endofjoke|>\n",
            "O que √© uma minha pessoa? √â uma minha t√° fase <|endofjoke|>\n",
            "Se O Tio do Thor o Thor que ficou doer que fazer... ...este uma bico... <|endofjoke|>\n",
            "O que √© um caminh√£o? √â amarelo. <|endofjoke|>\n",
            "Qual a semelhan√ßa entre o nome para a melhor na tia... ... na cabe√ßa n√£o faz. <|endofjoke|>\n",
            "Por que o √≠ndio pela fazer t√£o nadar? Porque o tio do g√© a √≥-rite. <|endofjoke|>\n",
            "Por que o melhor √© um jogo? Porque j√° acontecer o m√©dico <|endofjoke|>\n",
            "Voc√™ sabe porque ele √© chove? <|endofjoke|>\n",
            "O que √© um quisquier? <|endofjoke|>\n",
            "O que √© um quisquier? √â que √© s√≥ bom? ü§∑ü§Å <|endofjoke|>\n",
            "Por que n√£o √© o ativoso? Porque eles g√™r-pau-kau <|endofjoke|>\n",
            "Por que √© um caminh√£o? Por que\n",
            "\n",
            "[1010 | 4325.57] loss=2.39 avg=2.74\n",
            "[1020 | 4367.41] loss=2.25 avg=2.73\n",
            "[1030 | 4409.27] loss=2.42 avg=2.73\n",
            "[1040 | 4451.08] loss=2.24 avg=2.72\n",
            "[1050 | 4492.92] loss=2.42 avg=2.72\n",
            "[1060 | 4534.75] loss=2.34 avg=2.71\n",
            "[1070 | 4576.61] loss=2.45 avg=2.71\n",
            "[1080 | 4618.46] loss=2.37 avg=2.70\n",
            "[1090 | 4660.30] loss=2.24 avg=2.70\n",
            "[1100 | 4702.14] loss=2.05 avg=2.69\n",
            "[1110 | 4743.97] loss=2.35 avg=2.68\n",
            "[1120 | 4785.79] loss=2.20 avg=2.67\n",
            "[1130 | 4827.63] loss=2.33 avg=2.67\n",
            "[1140 | 4869.43] loss=2.27 avg=2.66\n",
            "[1150 | 4911.25] loss=2.14 avg=2.66\n",
            "[1160 | 4953.08] loss=2.51 avg=2.65\n",
            "[1170 | 4994.91] loss=2.13 avg=2.65\n",
            "[1180 | 5036.74] loss=2.12 avg=2.64\n",
            "[1190 | 5078.59] loss=2.06 avg=2.63\n",
            "[1200 | 5120.42] loss=2.17 avg=2.62\n",
            "======== SAMPLE 1 ========\n",
            "nd> !!! A √°lcerne no meu pequeno? <|endofjoke|>\n",
            "Se a sugua contato est√° em lugar no meio da Vela? O Poito no meio. <|endofjoke|>\n",
            "Qual √© o m√©dico preferido dos v√≠rdias? Ralh√£ <|endofjoke|>\n",
            "Qual a doen√ßa favorita da rex? R. V√≠rus <|endofjoke|>\n",
            "Quem √© o relacionamento entre o dia v√£o? O Sextu <|endofjoke|>\n",
            "O que a sucesso disse para 1 ? <|endofjoke|>\n",
            "Criaram √†sles a lo√ßa de v√≥ tava fotografia... Quando ele queria acabou. <|endofjoke|>\n",
            "Por que a sucesso disse que ficante disse? Pra se formar comida√ß√£o do mundo... <|endofjoke|>\n",
            "Voc√™ q vc √© um caixa que ele √© um dado? Com um caixa que √© um dado? <|endofjoke|>\n",
            "Minha m√£e √© o que ser amarelo? Aima, vamos l√° tem <|endofjoke|>\n",
            "Por que o Batman n√£o usa super-heroes? Porque ele tem mulheres da Batman. <|endofjoke|>\n",
            "Um sabonete com um cachorro quando l√°... O Capedan <|endofjoke|>\n",
            "J√° vi um pobhito... ...vo l√°, quando vai paga pau, seus apressam deixar um pobhe, vai o cachorro l√°. <|endofjoke|>\n",
            "N√£o fazem de um gordo, sempre deixar um pobre... ...porque quando a m√™sica n√£o sua cadeira. <|endofjoke|>\n",
            "Por que o aluno perdo joga n√£o consegue chamados... que quando qu'il √© dois gatos? Porque a segunda n√£o consegue <|endofjoke|>\n",
            "Qual jogador de game que era muito grande bem? O Pok√©mon. <|endofjoke|>\n",
            "Crieu vc ficava quando chove, muito filhos. <|endofjoke|>\n",
            "S√≥ chama um jukebox. Quando esperando <|endofjoke|>\n",
            "Por que os brasileiro chamado O que os chamados <|endofjoke|>\n",
            "Porque n√£o conseguiu nunca apenas, pode at√© o primeiro s√©rie? Porque fica de prisionaram. <|endofjoke|>\n",
            "O que √© que a √°gua se pode faz na bola? Ele canta na frente. <|endofjoke|>\n",
            "O que √© que o que √©? Um caixa... <|endofjoke|>\n",
            "Por que os brasileiro de Ucadinha N√£o conseguiu em s√£o bons d√°? <|endofjoke|>\n",
            "Aquele perna que sempre se encontrar a sua vida Eu estava o nome. <|endofjoke|>\n",
            "Sabe por que pessoas pessostas roubais? Porque perdeu um otto reosa, mas ladei em comunica. <|endofjoke|>\n",
            "Vamos n√£o consegue jogar l√° pode ser deixar de pessoas... ai muitos pediu no site um √≥eslav Kl√¥. <|endofjoke|>\n",
            "O que o Batman faz querem juntos? Batman Batman! <|endofjoke|>\n",
            "Aquelas tios est√£o as comunicais com uma nacional de recupera√ß√£o, mas n√£o entende tamb√©m <|endofjoke|>\n",
            "Eu sei em cima de arquit√µes no v√≥ bibada, com uma matem√°tica Mas n√£o consegue cima de arquit√µes, com uma matem√°tica <|endofjoke|>\n",
            "O que tem 5 deuz√¢\n",
            "\n",
            "[1210 | 5180.73] loss=2.20 avg=2.62\n",
            "[1220 | 5222.54] loss=1.99 avg=2.61\n",
            "[1230 | 5264.39] loss=2.16 avg=2.60\n",
            "[1240 | 5306.23] loss=2.22 avg=2.60\n",
            "[1250 | 5348.05] loss=2.15 avg=2.59\n",
            "[1260 | 5389.91] loss=2.04 avg=2.58\n",
            "[1270 | 5431.79] loss=2.13 avg=2.58\n",
            "[1280 | 5473.63] loss=2.07 avg=2.57\n",
            "[1290 | 5515.48] loss=1.71 avg=2.56\n",
            "[1300 | 5557.34] loss=1.69 avg=2.55\n",
            "[1310 | 5599.19] loss=2.13 avg=2.54\n",
            "[1320 | 5641.07] loss=2.19 avg=2.54\n",
            "[1330 | 5682.92] loss=2.25 avg=2.53\n",
            "[1340 | 5724.74] loss=2.01 avg=2.52\n",
            "[1350 | 5766.55] loss=1.99 avg=2.52\n",
            "[1360 | 5808.38] loss=2.10 avg=2.51\n",
            "[1370 | 5850.22] loss=2.16 avg=2.51\n",
            "[1380 | 5892.05] loss=2.18 avg=2.50\n",
            "[1390 | 5933.90] loss=1.98 avg=2.50\n",
            "[1400 | 5975.77] loss=1.54 avg=2.48\n",
            "======== SAMPLE 1 ========\n",
            " pio! <|endofjoke|>\n",
            "Sabe por que a vila no pote com pedenas? Porque ela tem muita pintista <|endofjoke|>\n",
            "J√° deixa com o espanhol otricado, qual o filme de m√∫sica? O filme de nascimento <|endofjoke|>\n",
            "Por que se atravessando em cima com uma piscina? Porque ela quero a piscineja <|endofjoke|>\n",
            "Qual o jornal da banda que chamava das pernas? √¢ia <|endofjoke|>\n",
            "Qual √© o tipo de novo e espa√±ol ? O tipo-da! <|endofjoke|>\n",
            "Porque o aplicativo giganza nova cria em falta? Porque ele r√©ptil de v√≠rus <|endofjoke|>\n",
            "Um arroz numa cachorro... N√£o como a corredor <|endofjoke|>\n",
            "Uma mulher ova minha brincar de um povo e pouco da minha fanta... Ele √© um quase ficou minha fanta e s√≥ lheiopo <|endofjoke|>\n",
            "Qual o nome da pessoa que as fricas matamos? √ìlhalo-pope <|endofjoke|>\n",
            "Caltarei para se for nov pra cumprir√° pessoas... Ent√£o viu um brimjo e ficou brilhante <|endofjoke|>\n",
            "Aulas √© um √≥culos pessoas e fic√¥s √© fofa cagar√ßo, matem√°tu n√£o sei e seguro paulha <|endofjoke|>\n",
            "Um pr√≥cracq uma an√£o geralta e voc√™ s√≥ chinescent e eu estou tomate de √¢nus Sem√£o e c√ßa as minhas pessoas. E uma cama√ß√∫car, vai sempre l√° √† noite <|endofjoke|>\n",
            "Quem √© a pessoa que √© o p√£o de jorquef√¥? O pita-joga <|endofjoke|>\n",
            "Qual carro ntoeste √© o carro de p√¥neiros? Carro √© dia <|endofjoke|>\n",
            "Qual √© a cantora que √© cantado? Ford F-back <|endofjoke|>\n",
            "A pessoa √© o carro de gordho d√° n√£o sobre. A tequila faz N√£o pode ter a pessoa n√£o gosta <|endofjoke|>\n",
            "Qual o queijo repetitivo para os men√∫do brasileis? O que j√° jango <|endofjoke|>\n",
            "Aquelas dor muitos s√£o paus. Tango s√£o palt√°-desistas, deveria o queijo pode v√™ <|endofjoke|>\n",
            "O que a a√ß√£o disse para a outra? e o que j√° tem poura? Ele n√£o ainda est√° bate e v√™ <|endofjoke|>\n",
            "Por que voc√™ fica com minha gato de tr√™s? Porque eu oo gato de gato. <|endofjoke|>\n",
            "Qual o fil√≥sofo que voc√™ gordho que o homem sem comunicar com fome de voc√™?  O fil√≥sofo <|endofjoke|>\n",
            "Qual √© o fil√≥sofo que voc√™ pode ser maconha? O fil√≥sofaulto <|endofjoke|>\n",
            "Uma mulher do escada vistoos que n√£o t√™m ajudaram. Uma maram nunca t√™ muito do corredor <|endofjoke|>\n",
            "Que sobrinho que n√£o tem um carro? Podou umas minhas. <|endofjoke|>\n",
            "Qual o problema de escolher que gosta de uma cidade entre p√≥lenhos e um s√≥ corona? O p√°lad√≥ 23. <|endofjoke|>\n",
            "Se eu n√£o falar de comentar o nuevo barbeu entra um tigre. Quer? Entra?\n",
            "\n",
            "[1410 | 6035.97] loss=2.08 avg=2.48\n",
            "[1420 | 6077.77] loss=1.92 avg=2.47\n",
            "[1430 | 6119.60] loss=1.98 avg=2.46\n",
            "[1440 | 6161.39] loss=2.00 avg=2.46\n",
            "[1450 | 6203.18] loss=2.08 avg=2.45\n",
            "[1460 | 6245.02] loss=1.97 avg=2.45\n",
            "[1470 | 6286.84] loss=1.99 avg=2.44\n",
            "[1480 | 6328.62] loss=2.02 avg=2.44\n",
            "[1490 | 6370.43] loss=2.19 avg=2.43\n",
            "[1500 | 6412.25] loss=1.94 avg=2.43\n",
            "Saving checkpoint/run1/model-1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5fe473-cd0f-4282-bb3f-42becf82bf97"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-1500\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf04c24-3a28-4bf4-d424-954af6df3e94"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aremen, November 3 (CNA) - A Colombian man was sentenced to 10-20 years in prison for evading an EU arrest warrant.\n",
            "\n",
            "Carlos Boulos, 40, of Amedeiros, was sent to a Maracay for 5.5 years. Mina Manqueir√£o, a social worker at Amedeiros hospital, said Boulos' crime was \"no crime at all\".\n",
            "\n",
            "\"No crime at all. No crime, no crime at all. I am a social worker, not a crime, no crime at all,\" Manqueir√£o said.\n",
            "\n",
            "READ MORE:\n",
            "\n",
            "* Evacueil de √Ågua n'ava atra√ß√£o de fazer em √¥nibus\n",
            "\n",
            "* Evacueil de √Ågua n'ava atra√ß√£o de s√≥ naquele l√≠derada? S√≥ computador√°, perguntaram de a√ß√£o de contra-inf√≥bienlandos e com todo mundo e manzar) (Por que uma biblioteca d√° um fusca tront√°rio)\n",
            "\n",
            "At a time when most crime in the continent is directed at the top, it is important to point out.\n",
            "\n",
            "\"No crime at all. No crime, no crime at all. I am a social worker, not a crime.\" -Carlos A. Boulos\n",
            "\n",
            "A remit of the interior and social protection and the interior justice so far is to decide on a term for Boulos' crime, and decide on a term for a crime at the interior, at the judicial and at the judicial and at the judicial and at the judicial and at the judicial and at the judicial and if not the judicial, at both for T√°tulo, and for Y√°calo, and at the judicial and therefor for Boulos. -iNEGR√ÅL RIETOS) (Amer√µes policia... √Ågua √≥ avisou Ocado! Psicologica-porque-todos) (Por que o grupo j√° n√£o quiser o t√™nima na rua) (Por que o corto quente n√£o t√™nima na rua) (Por que o corto t√™nima na rua) (Por que o corto t√™nima na rio) (Por que o corto t√™nima na rino) (Por que o corto t√™nima na ronto) (Por que o corto t√™nima na ronto)<|endoftext|>So, it's time for us to decide on a Wonder Woman! Batavia is a portmanteua. <|endofjoke|\n",
            "Pssst, Batavia, let us be Wonder Women. <|endofjoke|\n",
            "\"Gone But Ever\" is a missive of the original.\n",
            "\"Grammy\" is a missive from \"The Rock\" <|endofjoke|\n",
            "\"Bom das fosse faz\" is a missive from \"The Batman\" <|endofjoke|\n",
            "Para crias √© em mal\" is a missive from \"The Batman\" <|endofjoke|\n",
            "Para crias √© √© um bari novo? Para crias √© √© um bari e pqe que eu bari √© mais n√£o fazer na Batman e pqe que eu bari √© √© √© pqe dizia. <|endofjoke|\n",
            "\"Dia dia, mas estando seu m√£o coment√°rico\" <|endofjoke|\n",
            "Porque toda a semel de nacional fumaba? Porque ela √© um b√™badente. <|endofjoke|\n",
            "\"Por que o homem tem a alta de Thor?\" Porque ele √© \"Thor\". <|endofjoke|\n",
            "Porque o pedreiro tem um tigre\" ...\"tina fica lunda\". <|endofjoke|\n",
            "Quando o √∫ltimo vem a prostituta, mas queria est√° dif√≠cil pra parar. <|endofjoke|\n",
            "Sabe porque o jacar√© que n√£o precisa a namorada? Porque ele se est√° no bom sistema. <|endofjoke|\n",
            "O que o brigade √© terminador para v√™? S√≥ a brigade √© falta. <|endofjoke|\n",
            "Por que o cachorro n√£o quis amarelo? Porque t√° sempre enquanto no caminho. <|end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "outputId": "a9738910-e34b-4c12-8c1b-33ee52b8ffe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=140,\n",
        "              temperature=0.8,\n",
        "              top_k=40,\n",
        "              prefix='Quando',\n",
        "              truncate='<|endofjoke|>',\n",
        "              nsamples=10,\n",
        "              batch_size=1,\n",
        "              )"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quando a pessoa atravessar com o pia ele! \n",
            "====================\n",
            "Quando para l√°, uma coisa. \n",
            "====================\n",
            "Quando estava que ele se chama um dia. \n",
            "====================\n",
            "Quando estou e aparecida? A com falta \n",
            "====================\n",
            "Quando a dor em casa? O Corrinho \n",
            "====================\n",
            "Quando estava a meijo? Porque ela estava a lavender. \n",
            "====================\n",
            "Quando quisquando estava no meio do natal? Uma filha e ainda no tributa do c√©u, a comprar o nome. \n",
            "====================\n",
            "Quando, no churrasco no bar? O churrasco no c√©u! \n",
            "====================\n",
            "Quando √© espirram? Na frita. \n",
            "====================\n",
            "Quando! \n",
            "====================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD"
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}