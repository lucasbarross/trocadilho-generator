{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "##  Trocadilho Generator \n",
        "\n",
        "Notebook modified from the original by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read this [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use the original notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_"
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1f2b28-69fe-47a2-b933-5c9f104bc4db"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov  2 20:01:22 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de94ebfc-b1d9-4e21-c025-67a263e82578"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 409Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 7.32Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 740Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:10, 47.6Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 524Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 10.1Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 12.2Mit/s]                                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d220c773-acff-4743-cc7a-894c29bfe12d"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Downloading a Text File to be Trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/lucasbarross/trocadilho-generator/main/training/jokes.txt\"\n",
        "data = requests.get(url)\n",
        "file_name = \"jokes.txt\"\n",
        "\n",
        "with open(file_name, 'w') as f:\n",
        "  f.write(data.text)\n",
        "\n",
        "gpt2.copy_file_from_gdrive(\"../../\"+file_name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "The next cell will start the actual finetuning/training of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b51720-5b5a-4738-e09c-fda8fdf16066"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1470.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 13288 tokens\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10 | 48.76] loss=3.82 avg=3.82\n",
            "[20 | 92.44] loss=3.32 avg=3.57\n",
            "[30 | 136.07] loss=2.74 avg=3.29\n",
            "[40 | 179.70] loss=2.21 avg=3.02\n",
            "[50 | 223.27] loss=1.37 avg=2.68\n",
            "[60 | 266.82] loss=0.92 avg=2.38\n",
            "[70 | 310.33] loss=0.50 avg=2.10\n",
            "[80 | 353.74] loss=0.31 avg=1.87\n",
            "[90 | 397.21] loss=0.28 avg=1.69\n",
            "[100 | 440.79] loss=0.11 avg=1.52\n",
            "[110 | 484.29] loss=0.08 avg=1.38\n",
            "[120 | 527.83] loss=0.07 avg=1.27\n",
            "[130 | 571.35] loss=0.06 avg=1.17\n",
            "[140 | 614.88] loss=0.04 avg=1.08\n",
            "[150 | 658.39] loss=0.15 avg=1.02\n",
            "[160 | 701.92] loss=0.04 avg=0.95\n",
            "[170 | 745.46] loss=0.04 avg=0.89\n",
            "[180 | 789.01] loss=0.04 avg=0.84\n",
            "[190 | 832.53] loss=0.03 avg=0.79\n",
            "[200 | 876.09] loss=0.04 avg=0.75\n",
            "======== SAMPLE 1 ========\n",
            "nou para as subiu em 1? As subiu-iz√©.\n",
            "Qual √© a fruta que √© o super-her√≥i do super-her√≥i do super-smaragda? O Li M√£o.\n",
            "Qual √© o frango que as outras povas fugroneira? Os ma-feios.\n",
            "Qual √© o jogador que as atores vov√¥s est√°s? O jogador-dos.\n",
            "Por que  o filho do filho? Porque ele √© filho-doke.\n",
            "Por que o surfista n√£o √© l√≠quido? Porque n√£o √© l√≠quido\n",
            "Qual √© o fen√¥meno que acontece no mar que o Thor tem medo? Thorniquete.\n",
            "Qual o animal que mais gosta de jogar futebol? O goooooooooolfinho.\n",
            "Tinha um pontinho verde no Xbox, qual √© o nome do jogo? Assassins GreenD.\n",
            "Por que  o banheiro n√£o √© considerado um carro? Porque ele era um jogo de tolerado.\n",
            "Por que a vaca foi pra Alemanha? Porque era po-pi√™.\n",
            "Por que o magel voade o irm√£e a√ßa√≠ disse para a cabe√ßa que se chama voc√™ tem o outro. Voc√™ sabe v√£o √† t√£o de calor.\n",
            "Qual √© o o celular que o eletrodom√©stico da sele√ß√£o brasileira de passatos? Celular.\n",
            "Qual √© o oomma corpo do oceano? Opta.\n",
            "Qual o anedatics que anda mundo de tolerado. Comque no escrever mais depressa sem aponta onde as pessoas est√£o? Aponta-demandas.\n",
            "Por que a loira joga o rel√≥gio pela janela? Porque ela √© morre a Holanda.\n",
            "O que √© que √© √© √© que depois de ficar com os cartas mais depressaendas? Comer t√™m o Peter Cheque.\n",
            "Por que √© que o depressa √© precipitada pela janela? Porque ele √© precipitada el√©trica.\n",
            "Qual √© a parte do quarto que o Thor mais gosta de contar? Thor-ist√≥scn√≠cio.\n",
            "Tinha um salo de um of√≠cia com se chama Ch√°-seis isola. Qual o nome do desenho todo quarto? Sonja-ingredinho.\n",
            "Qual √© o desenho todos los saudades? Ah, Dien üëú‚Äç‚ôØÔ∏è.\n",
            "Qual o alimento de entregard todos los gostam? And so on...\n",
            "O que √© que se adapta decem cima no espa√ßo? O que √© que quando b√™bado o seuugu empolar?\n",
            "Qual √© o pa√≠s onde as pessoas mais praticam muscula√ß√£o? S√≥-malha.\n",
            "Por que o vaso √© considerado um ver Muuuuu-nique? Porque √© dez√£o s√≥ prescrever.\n",
            "Por que o vaso √© considerado um sao-cho? Porque ele √© dez√£o s√≥ um sao-dia.\n",
            "Qual √© o p√£o que o Batman √© considerado um cachorro? Bat-deia.\n",
            "Qual √© o animal que mais as pessoas mais praticam? Ame-pai.\n",
            "Qual animal √© a capital que beija todo mundo? Mendo√ßa.\n",
            "Qual √© o order que sempre absolur quem √© o nome do koch ‚Äï √© o lugar de dois d√° pra se favorita de cada vez? Muuuuuuuuuuu-a.\n",
            "Qual animal √© o nome da norma jur√≠dica ‚Äï √© o nome da femin√≥grafo n√£o rel√≥gio √© para ser um peixe? Laba-fe-c√°ve.\n",
            "Qual √© a capital entre o √≥rg√£o mais entende? Uau-fe-c√°ve.\n",
            "Qual √© a comida que tem problema, e o tom-de-les? En-Churo.\n",
            "Por que nos montes mais altos e frios da Su√≠√ßa tem cachorros? Porque l√° est√£o os au-aupes.\n",
            "Qual √© o frango que √© o idoso quando est√° na rua? O brinquedo\n",
            "\n",
            "[210 | 939.92] loss=0.03 avg=0.72\n",
            "[220 | 983.45] loss=0.03 avg=0.68\n",
            "[230 | 1027.04] loss=0.02 avg=0.65\n",
            "[240 | 1070.61] loss=0.03 avg=0.62\n",
            "[250 | 1114.14] loss=0.03 avg=0.59\n",
            "[260 | 1157.61] loss=0.05 avg=0.57\n",
            "[270 | 1201.16] loss=0.04 avg=0.55\n",
            "[280 | 1244.69] loss=0.02 avg=0.53\n",
            "[290 | 1288.27] loss=0.02 avg=0.51\n",
            "[300 | 1331.82] loss=0.03 avg=0.49\n",
            "[310 | 1375.37] loss=0.03 avg=0.47\n",
            "[320 | 1418.88] loss=0.06 avg=0.46\n",
            "[330 | 1462.33] loss=0.03 avg=0.44\n",
            "[340 | 1505.76] loss=0.02 avg=0.43\n",
            "[350 | 1549.20] loss=0.03 avg=0.41\n",
            "[360 | 1592.63] loss=0.02 avg=0.40\n",
            "[370 | 1636.05] loss=0.03 avg=0.39\n",
            "[380 | 1679.48] loss=0.02 avg=0.38\n",
            "[390 | 1722.90] loss=0.02 avg=0.36\n",
            "[400 | 1766.33] loss=0.02 avg=0.35\n",
            "======== SAMPLE 1 ========\n",
            " au carro, volta e garai a bem t√™nis? Porque mora no Brasil parro je nei quando se esque.\n",
            "Qual a se-cores que os viciados em qualquer pront the galax√£o? Gisele Print.\n",
            "Qual a dos maquilos de tom√° pra fazer? Gisele Albeiro.\n",
            "Qual a comida que tem um cachorro-quente? A clique.\n",
            "Por que a rua au Sarmiento tem n√£o pote em p√≥ se declara? Porque ela √© um cachorro-quente.\n",
            "Qual o ve√≠culo de comunica√ß√£o que sempre estavam em ace enrijecida? Jabuti-Cabra.\n",
            "Por que um homem vivia cheio de velas em sua casa? Porque ele queria achar o tesouro.\n",
            "Qual a fruta que termina primeiro do ferreiro? O golfer-a.\n",
            "Por que a monkia habo a cama do porque ela ditaba? Porque ele punho a vaca.\n",
            "Qual √© a explos√£o do Thor? O m√©dicoThor.\n",
            "O que o zero √© que o m√©dico febou delauno e o outro.\n",
            "Qual o carro que fazer interfixa? Cooper.\n",
            "Por que a vaca foi pra Alemanha? Porque ele queria achar o tesouro.\n",
            "Qual o as senhor do Thor? FoTHormode.\n",
            "Qual a haste de metal que diz estavam com picol√©s? Dynamite.\n",
            "Qual a haste de metal juntar um pontinho delas? Rodrigo Faro.\n",
            "Qual √© o traficante que mais gosta de flores? Cast-a-Coca.\n",
            "Qual √© a parte mais feminina da casa? JanELA.\n",
            "Por que o cachorro o seu cachorro-pode? Porque ele queria astr√¥nomo desencapado.\n",
            "Qual o contr√°rio de cachorro-pens? Porque ele era dia de cheia.\n",
            "Represento o amor, mas amor n√£o posso ter, me desenh ofto e um cavalheiro, mais amado muskota que no interior o tamb√©m √© do as chap√©iros.\n",
            "O que o zero dise para o meusquier 1 que o zero estudiante 2? Agulha.\n",
            "O que passa o pr√≥ximais que se o rico comer ele morre? Agulha.\n",
            "O que o carro estava no tr√™sse? Engata.\n",
            "Por que o cachorro ia communo n√£o gosta da casa? Porque ela √© engata fecha a novela.\n",
            "Por que no barco de futebol n√£o quem √© empatado estava o demais? Porque elas √© empatado um clipe.\n",
            "Qual o ve√≠culo que lava roupas? Tanque.\n",
            "Qual √© a diferen√ßa entre a cacha√ßa e a mulher? A cacha√ßa d√° dor de cabe√ßa s√≥ um dia.\n",
            "Por que o macaco tem medo de martelo? Porque ele √© um macaco-prego.\n",
            "Por que o jogador n√£o estava conseguindo ligar do campo? Porque ele estava fora de cachorro.\n",
            "Qual o ve√≠culo que o M√©dico mais pregui√ßoso que tem no TV? Muuuuri√ßoca.\n",
            "Por que o jogador n√£o estava acabou carnive? Porque ele estava coronir√£o.\n",
            "Qual √© o hidrocarboneto da raiva? Hadao filho.\n",
            "Qual √© a vaca que molhar tra outra? Da Thornea.\n",
            "Por que o maior dramaturgo de l√≠ngua inglesa? Porque ela √© l√≠ngua a m√£e a√ßa√≠.\n",
            "Qual o contr√°rio de papelada? Porque ele quer que t√° sempre chie.\n",
            "N√£o perdido de bacon √© o brinquedo? √â o brinquedo de volta a-d√°.\n",
            "Qual o time mais amado da gama de cantar? Caetano De Caesarean.\n",
            "Qual a ave que are causez-am mas? Fazem-Z√©.\n",
            "Por\n",
            "\n",
            "[410 | 1828.47] loss=0.02 avg=0.34\n",
            "[420 | 1871.98] loss=0.02 avg=0.34\n",
            "[430 | 1915.46] loss=0.03 avg=0.33\n",
            "[440 | 1958.91] loss=0.02 avg=0.32\n",
            "[450 | 2002.34] loss=0.02 avg=0.31\n",
            "[460 | 2045.81] loss=0.02 avg=0.30\n",
            "[470 | 2089.29] loss=0.02 avg=0.29\n",
            "[480 | 2132.77] loss=0.02 avg=0.29\n",
            "[490 | 2176.21] loss=0.02 avg=0.28\n",
            "[500 | 2219.72] loss=0.02 avg=0.27\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 2266.09] loss=0.02 avg=0.27\n",
            "[520 | 2309.55] loss=0.03 avg=0.26\n",
            "[530 | 2352.98] loss=0.02 avg=0.26\n",
            "[540 | 2396.40] loss=0.02 avg=0.25\n",
            "[550 | 2439.84] loss=0.02 avg=0.24\n",
            "[560 | 2483.31] loss=0.02 avg=0.24\n",
            "[570 | 2526.76] loss=0.02 avg=0.23\n",
            "[580 | 2570.24] loss=0.02 avg=0.23\n",
            "[590 | 2613.70] loss=0.02 avg=0.22\n",
            "[600 | 2657.15] loss=0.02 avg=0.22\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "Thor: Ragnarok\n",
            "O que o zero disse para o s√≥ futebol? Nossa.\n",
            "Qual √© o aparelho pelo que trabalha com montaria? Carro-o-leito.\n",
            "Qual √© o pato que correce de futebol so far as regards audio? Corre-sca.\n",
            "Qual √© o animal que mais gosta de flores? A fo-foca.\n",
            "Por que o folhada estava quando acordasse fizesse? Porque l√° fase buzinou.\n",
            "Qual √© o ap√≥sio afficionado que tem tudo? Tony Ramos.\n",
            "Qual √© a especializa√ß√£o do Mundo? Para fazer.\n",
            "O que o zero dise para o oito? Nossa.\n",
            "Qual a cantora que mais gosta de flores? Ama-gadinha.\n",
            "Qual √© o carro mais fora muito paradiso? Geraldo √Ålcool-Em-Mim.\n",
            "Qual a carro conta tudo? Nome.\n",
            "Por que os c√£es da Austr√°lia ficam roucos? Porque a capital do pa√≠s se chama c√£o-berra.\n",
            "Por que os pap√©is e as folhos gostam de casas de raz√£o? Porque eles folhos raz√£o.\n",
            "Qual a parte mais do mundo mais gosta de flores? Assassins Farm.\n",
            "Qual √© o eletrodom√©stico do vulc√£o? Domestique Vim-Pool.\n",
            "Qual o perguntou se ele gostava da Grisw√°? Pablo Salva-z-Ui-Ceno.\n",
            "Qual √© o frango que est√° sempre na mesa? O frango alinamesa.\n",
            "Qual √© a profiss√£o do Thor? CorreThor.\n",
            "O que o Thor mais gosta de flores? Carlos Alber-thor (Carlos Alberto)\n",
            "Qual √© a parte mais do jogo que os 4 filhas? Jogo de flores.\n",
            "O que √© um grande entre aranha e um pregui√ßoso? √â um grande entre au-taco.\n",
            "Qual √© o hanna que mais gosta de flores? Carlos Alber-thor (Carlos Alberto)\n",
            "Por que estava batendo a capital do Equador? Porque era um capital do 3-D-itation.\n",
            "Qual √© o fen√¥meno que acontece no mar que o Thor tem medo? Thormenta.\n",
            "Qual √© o animal que mais gosta de flores? Carlos Alber-thor (Carlos Alberto)\n",
            "Qual √© a profiss√£o do Thor? CorreThor.\n",
            "Qual √© o animal que gosta de flores? Carlos Alber-thor (Carlos Alberto)\n",
            "Eu tenho um cavalheiro que o Thor tem eu fim? His-THOR-e-i-kara.\n",
            "Qual o doce favorito do Thor? Thortuguita.\n",
            "Por que um 10ago no tudo? Porque ele n√£o tenho um ladhor tudo.\n",
            "O que o zero dise para o oito? Nossa.\n",
            "O que o zero dise para foi fazer no espa√ßo? Foi cantar p√° god (Pagode)\n",
            "Qual √© o estado preferido do Thor? THORude.\n",
            "Qual a semelhan√ßa entre aranha e escorpi√£o? Com nenhum dos dois d√° pra fazer bolo de abacate.\n",
            "Meu av√¥ tem 4 filhas, cada filho tem 4 filhas. Quantos primos eu tenho? 12. Tr√™s s√£o meus irm√£os e um sou eu.\n",
            "Eu tenho do eu sou? Eu gasem t√™th.\n",
            "Eu tenho do germ√°rio? Eu tigre.\n",
            "Eu tenho dos mejores? Eu tigre-grafia.\n",
            "Eu tenho dente de bicicleta? Eu teclado.\n",
            "Eu tenho lobo de tomar ch√°? Eu teclado.\n",
            "Eu fimososo eu revocar tamb√©m mant√≥rez? Nu hochardende.\n",
            "Por que o Batman mais gosta de fazer? Porque ele √© paciente.\n",
            "Qual √© o time mais regal-doce? Tony.\n",
            "Qual √© a group habit√£o que o Thor tem medo? Thormenta.\n",
            "Represento o\n",
            "\n",
            "[610 | 2719.45] loss=0.02 avg=0.22\n",
            "[620 | 2762.92] loss=0.03 avg=0.21\n",
            "[630 | 2806.39] loss=0.02 avg=0.21\n",
            "[640 | 2849.86] loss=0.02 avg=0.20\n",
            "[650 | 2893.33] loss=0.02 avg=0.20\n",
            "[660 | 2936.80] loss=0.02 avg=0.20\n",
            "[670 | 2980.26] loss=0.02 avg=0.19\n",
            "[680 | 3023.71] loss=0.02 avg=0.19\n",
            "[690 | 3067.14] loss=0.02 avg=0.19\n",
            "[700 | 3110.59] loss=0.01 avg=0.18\n",
            "[710 | 3154.02] loss=0.02 avg=0.18\n",
            "[720 | 3197.43] loss=0.02 avg=0.18\n",
            "[730 | 3240.90] loss=0.01 avg=0.17\n",
            "[740 | 3284.32] loss=0.01 avg=0.17\n",
            "[750 | 3327.74] loss=0.02 avg=0.17\n",
            "[760 | 3371.21] loss=0.02 avg=0.16\n",
            "[770 | 3414.66] loss=0.02 avg=0.16\n",
            "[780 | 3458.10] loss=0.02 avg=0.16\n",
            "[790 | 3501.56] loss=0.02 avg=0.16\n",
            "[800 | 3545.02] loss=0.02 avg=0.15\n",
            "======== SAMPLE 1 ========\n",
            "raado.\n",
            "O que o milho disse para a pipoca? HuMILHO mesmo.\n",
            "Qual √© o doce preferido do Thor? Thorrone.\n",
            "Qual √© o prato preferido do Thor? Thorresmo.\n",
            "O que o Thor faz quando corta o dedo? Thorniquete.\n",
            "O que acontece se o Thor assoprar? THORnado.\n",
            "Quantas pata tem uma pata? Somente uma, se fosse mais seria patapatapatapata...\n",
            "Qual √© a semelhan√ßa entre aranha e escorpi√£o? Com nenhum dos dois d√° pra fazer bolo de abacate.\n",
            "Meu av√¥ tem 4 filhos, cada filho tem 4 filhos. Quantos primos eu tenho? 12. Tr√™s s√£o meus irm√£os e um sou eu.\n",
            "Ela tinha 4 filhos. Janeiro, Fevereiro, Mar√ßo. Qual √© o nome do quarto filho. O nome da crian√ßa e 'Qual'.\n",
            "Qual o esporte que os cientistas gostam? F√≥rmula 1.\n",
            "Qual a profiss√£o hist√≥rica do Thor? His-THOR-i√≥grafo.\n",
            "Qual a cat√°strofe que o Thor tem medo? TerremoTHOR.\n",
            "O que o gato faz quando est√° na rua? Engata.\n",
            "Qual √© a profiss√£o do Thor? Mo-THOR-iz-oca.\n",
            "Qual a especializa√ß√£o do Thor? PaTHORlogia.\n",
            "Qual a profiss√£o secreta do Thor? InspeThor.\n",
            "Qual √© o frango que est√° sempre na mesa? O frango alinamesa.\n",
            "Qual √© a profiss√£o do Thor? Thoreiro.\n",
            "Qual o p√£o que o Thor mais come? O THORrado.\n",
            "No carro estavam 1 av√≥, 2 pais, 2 filhos e 1 neto. Quantas pessoas estavam no carro? 3 pessoas.\n",
            "O que √© surdo e mudo mas conta tudo? Livro.\n",
            "Por que a fra√ß√£o n√£o cabe em uma casa? Por que ela tem 4 quarto.\n",
            "Qual a parte do corpo que Thor mais gosta? Thornozelo.\n",
            "O que o zero disse para o seis? Nossa, apertou tanto o cinto que abriu a boca.\n",
            "Qual e o animal que mais gosta de jogar futebol? GOLfinho.\n",
            "Qual √© o carro que s√≥ anda bem vestido? Blazer.\n",
            "Por que guarda de tr√¢nsito √© o ser mais forte do mundo? Porque ele pode parar carros s√≥ com uma das m√£os.\n",
            "Qual √© o fen√¥meno que acontece no mar que o Thor tem medo? Thormenta.\n",
            "Qual √© o animal que mais gosta de jogar futebol? O goooooooooolfinho.\n",
            "Tinha um pontinho verde no Xbox, qual √© o nome do jogo? Assassins GreenD.\n",
            "Por que n√£o podemos levar c√£es pros Estados Unidos? Porque l√° fura-c√£o.\n",
            "Qual √© o pa√≠s feminino? VenezuELA.\n",
            "Qual a medita√ß√£o que se faz acordado? ONnnn...\n",
            "O que a legume √© do filho do filho dela? Av√≥bora.\n",
            "O que o zero dise para o oito? Nossa, que cinto apertado!\n",
            "O que passa por todas as casas mas n√£o sai do lugar? A rua.\n",
            "Cinco pol√≠ticos estavam em uma lancha que acabou virando no meio do mar. Voc√™ sabe como salv√°-los? N√£o? Perfeito!\n",
            "Qual √© o bicho que anda com as patas? O pato.\n",
            "O que o Thor gosta de ser no tempo livre? Fo-THOR-grafo.\n",
            "Por que as mulheres n√£o sentem frio? Porque elas est√£o cobertas de raz√£o.\n",
            "Por que em Minas Gerais n√£o tem mar? Porque l√° eles rezam livrai-nos de todos os mar.\n",
            "Qual o animal que a\n",
            "\n",
            "[810 | 3607.22] loss=0.02 avg=0.15\n",
            "[820 | 3650.66] loss=0.01 avg=0.15\n",
            "[830 | 3694.09] loss=0.01 avg=0.15\n",
            "[840 | 3737.51] loss=0.02 avg=0.14\n",
            "[850 | 3780.97] loss=0.02 avg=0.14\n",
            "[860 | 3824.42] loss=0.01 avg=0.14\n",
            "[870 | 3867.84] loss=0.01 avg=0.14\n",
            "[880 | 3911.27] loss=0.02 avg=0.14\n",
            "[890 | 3954.69] loss=0.01 avg=0.13\n",
            "[900 | 3998.11] loss=0.01 avg=0.13\n",
            "[910 | 4041.50] loss=0.02 avg=0.13\n",
            "[920 | 4084.99] loss=0.02 avg=0.13\n",
            "[930 | 4128.44] loss=0.01 avg=0.13\n",
            "[940 | 4171.86] loss=0.01 avg=0.12\n",
            "[950 | 4215.31] loss=0.02 avg=0.12\n",
            "[960 | 4258.72] loss=0.02 avg=0.12\n",
            "[970 | 4302.15] loss=0.02 avg=0.12\n",
            "[980 | 4345.57] loss=0.02 avg=0.12\n",
            "[990 | 4388.96] loss=0.02 avg=0.12\n",
            "[1000 | 4432.35] loss=0.01 avg=0.11\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6965eb-bfa8-44ef-c32c-f3dcb4155e20"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-1000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824c86c4-ade3-460e-d15c-c4e03d97da03"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qual √© a comida mais devagar? A devagar√ßo.\n",
            "O que acontece se o coqueiro beber demais? Ele leva o burro e de um cachorro.\n",
            "Qual o ator que livra as pessoas das dores? Malvino Salva-dor.\n",
            "Por que as vogais A, E, I e O foram rejeitadas pela banda Scorpions? Porque eles Still Love U.\n",
            "Qual a atriz mais enxuta? Deborah Secco.\n",
            "Qual o santo do Rodrigo? S√£o Toro.\n",
            "Qual enfermidade tem o t√©cnico da sele√ß√£o brasileira de futebol quando entra em um labirinto? Labirin-Tite.\n",
            "Qual o jogador que s√≥ vive com raiva? Bravo.\n",
            "O que todo homem faz quando est√° no banheiro? Sai do banheiro.\n",
            "Por que o nadador jogou a televis√£o na piscina? Para fazer um nado sintonizado.\n",
            "Por que o italiano tem que se equilibrar jogando xadrez? Porque sen√£o ele perde sua torre.\n",
            "Por que a cama do sab√£o facilmente estoura? Porque seu colch√£o √© de espuma.\n",
            "Qual √© o eletrodom√©stico preferido do Batman? Bat-deira.\n",
            "Qual comida que quase tira um 10? Strogo-nove.\n",
            "Qual o brinquedo preferido do McDonald's? McSteel.\n",
            "O que a vaca foi fazer na papelaria? Comprar uma muuuuuuuu-chila.\n",
            "Qual o material escolar que mostra a parte de seu corpo que d√≥i? Aponta-dor.\n",
            "Qual a rede social mais gorda? InstaGrama.\n",
            "Qual √© o tipo de festa que os cegos frequentam? O braille funk.\n",
            "Por que no Rio de Janeiro as pessoas n√£o comem p√£o de sal? Porque eles t√™m o P√£o de A√ß√∫car.\n",
            "Qual super her√≥i tira foto no escuro? Flash.\n",
            "Quando est√° apaixonado, como um sab√£o em p√≥ se declara? Eu te OMO.\n",
            "O que a m√£e a√ßa√≠ disse para os outros a√ßa√≠s? O √∫ltimo a√ßa√≠ feche a porta.\n",
            "Qual √© o carro que gosta de fazer exerc√≠cios? Cooper.\n",
            "Por que o pai do Thor √© um ser pr√©-hist√≥rico? Porque ele √© Odinossauro.\n",
            "O que o Batman faz com o celular? Bat-Selfie.\n",
            "Qual o contr√°rio de futsal? Fut-a√ß√∫car.\n",
            "Como seria o nome chin√™s do pregui√ßoso? Kan-sei ou kochi-lin.\n",
            "Por que o rel√≥gio √© popular? Porque ele √© da hora.\n",
            "Por que a piada da bola √© a melhor? Porque ela √© bem bolada.\n",
            "Qual √© o traficante que toma uma gelada no boteco? Pablo Skol Bar.\n",
            "O que tem 5 cordas e uma cova? Viol√£o.\n",
            "Qual o santo da Ivete? S√£o Galo.\n",
            "Qual √© o chocolate dos gatos? Kit-Cat.\n",
            "Qual o esporte preferido dos ga√∫chos? Basquetch√™.\n",
            "Quem √© o rei da farm√°cia? Rei-m√©dio.\n",
            "Qual o contr√°rio de buscap√© Levam√£o.\n",
            "Qual o carro mais amado da Julieta? Alfa-Romeu.\n",
            "Qual a cerveja preferida dos mineiros? Bud-Uai-Z√©.\n",
            "Por que a Holanda n√£o cresce? Porque ela √© um Pa√≠s-Baixo.\n",
            "Qual o contr√°rio de cachorro-quente? Cadela fria.\n",
            "Por que o frango atravessou a rua? Porque era dia de folga da galinha.\n",
            "Por que o Thor √© rico? Porque ele tem O-din-din.\n",
            "O que √© o qu√™ √© e branco n√£o √© l√≠quido e serve pra beb√™? Fralda.\n",
            "Qual √© o ve√≠culo que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"LORD\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2"
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/774M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
            "\n",
            "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
            "====================\n",
            "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
            "====================\n",
            "The secret of life is in the universe.\n",
            "\n",
            "\n",
            "-\n",
            "\n",
            "The Red Devil\n",
            "\n",
            "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
            "\n",
            "\n",
            "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
            "====================\n",
            "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
            "\n",
            "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
            "\n",
            "As a scientist, I'm fascinated by the question of how life first began.\n",
            "\n",
            "It's the question that drives my work and the work of the scientists who work on it.\n",
            "\n",
            "My current research is exploring how microbes work in the first moments\n",
            "====================\n",
            "The secret of life is the journey of life, the search for the truth.\n",
            "\n",
            "4.4.2. The last thing you know\n",
            "\n",
            "There is nothing more important than the last thing you know.\n",
            "\n",
            "4.4.3. The little things that make all the difference\n",
            "\n",
            "The little things that make all the difference.\n",
            "\n",
            "4.4.4. The truth is the best teacher\n",
            "\n",
            "The truth is the best teacher.\n",
            "\n",
            "4.4.5. The truth is what\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}