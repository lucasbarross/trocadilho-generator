{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "##  Trocadilho Generator \n",
        "\n",
        "Notebook modified from the original by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read this [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use the original notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_"
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6ceff6-3297-4977-e101-31765b0bb4c7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  7 21:02:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cd3288-54ca-474d-ffc0-27b6b71dc22f"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 471Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 966kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 573Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:11, 6.97Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 519Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 1.18Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 1.15Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca17182-bde2-4f51-fcdb-d736f5ce19bf"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Downloading a Text File to be Trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/lucasbarross/trocadilho-generator/main/training/jokes.txt\"\n",
        "data = requests.get(url)\n",
        "file_name = \"jokes.txt\"\n",
        "\n",
        "with open(file_name, 'w') as f:\n",
        "  f.write(data.text)\n",
        "\n",
        "gpt2.copy_file_from_gdrive(\"../../\"+file_name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "The next cell will start the actual finetuning/training of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800b888d-d71c-4c62-8dd7-bbc8fa3e77b3"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              learning_rate=1e-5,\n",
        "              model_name='124M',\n",
        "              steps=1500,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 253358 tokens\n",
            "Training...\n",
            "[10 | 45.54] loss=4.12 avg=4.12\n",
            "[20 | 87.10] loss=3.70 avg=3.91\n",
            "[30 | 128.70] loss=3.81 avg=3.88\n",
            "[40 | 170.27] loss=3.94 avg=3.89\n",
            "[50 | 211.83] loss=3.89 avg=3.89\n",
            "[60 | 253.36] loss=3.57 avg=3.84\n",
            "[70 | 294.73] loss=3.59 avg=3.80\n",
            "[80 | 336.49] loss=3.49 avg=3.76\n",
            "[90 | 378.20] loss=3.72 avg=3.75\n",
            "[100 | 420.13] loss=3.47 avg=3.73\n",
            "[110 | 462.18] loss=3.50 avg=3.70\n",
            "[120 | 504.25] loss=3.38 avg=3.67\n",
            "[130 | 546.32] loss=3.44 avg=3.66\n",
            "[140 | 588.28] loss=2.97 avg=3.60\n",
            "[150 | 630.29] loss=3.35 avg=3.59\n",
            "[160 | 672.28] loss=3.24 avg=3.56\n",
            "[170 | 714.28] loss=3.37 avg=3.55\n",
            "[180 | 756.24] loss=3.31 avg=3.54\n",
            "[190 | 798.18] loss=3.29 avg=3.52\n",
            "[200 | 840.11] loss=3.23 avg=3.51\n",
            "======== SAMPLE 1 ========\n",
            "s are better because we eat and do our job, better because we have more time. And so for me it was as if people don't care at all about, and not just for, a football player's time. So when he got his contract, it was just for him. It's a bit strange, because it makes you do better.\"\n",
            "\n",
            "In other words, Neymar's time is better for Barcelona than for L.S. Neymar's time is better for L.S. Neymar's time is better for L.S.\n",
            "\n",
            "Neymar was a starlet for Barcelona and L.S. Neymar is a starlet for L.S. It appears that there's also some debate...\n",
            "\n",
            "\"You are better at a footballer's time, O Jesus, O Jesus. Because L.S. Neymar is not a starlet at football club. O Jesus. Jesus ... Jesus.\"\n",
            "\n",
            "This is L.S.'s first top-end move. L.S. Neymar is just a starlet at a football club. O Jesus, and L.S.\" O Jesus, O Jesus, O Jesus ... Jesus ... Jesus ... Jesus ... Jesus ...!\"\n",
            "\n",
            "Pokalou! Here's what happens...\n",
            "\n",
            "\"L.S. Neymar is a starlet at a football club in Lidrome. ... Jesus! ... Jesus! ... Jesus!\" O Jesus, O Jesus, O Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ...!\" ... O Jesus, O Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ...!\" O Jesus, O Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ... Jesus ...\n",
            "\n",
            "[210 | 901.35] loss=3.08 avg=3.48\n",
            "[220 | 943.34] loss=3.15 avg=3.47\n",
            "[230 | 985.30] loss=3.17 avg=3.45\n",
            "[240 | 1027.21] loss=3.00 avg=3.43\n",
            "[250 | 1069.17] loss=3.07 avg=3.41\n",
            "[260 | 1111.13] loss=3.09 avg=3.40\n",
            "[270 | 1153.10] loss=3.02 avg=3.38\n",
            "[280 | 1195.06] loss=3.01 avg=3.37\n",
            "[290 | 1237.04] loss=3.00 avg=3.35\n",
            "[300 | 1279.01] loss=3.10 avg=3.34\n",
            "[310 | 1320.99] loss=2.90 avg=3.33\n",
            "[320 | 1363.00] loss=3.01 avg=3.32\n",
            "[330 | 1404.99] loss=2.67 avg=3.29\n",
            "[340 | 1446.98] loss=3.04 avg=3.28\n",
            "[350 | 1488.93] loss=3.03 avg=3.28\n",
            "[360 | 1530.86] loss=2.99 avg=3.27\n",
            "[370 | 1572.78] loss=3.15 avg=3.26\n",
            "[380 | 1614.74] loss=3.00 avg=3.25\n",
            "[390 | 1656.70] loss=3.04 avg=3.25\n",
            "[400 | 1698.58] loss=2.91 avg=3.24\n",
            "======== SAMPLE 1 ========\n",
            "|\n",
            "\n",
            "Jovem: \"Meu vai\". Não muda. <|endofjoke|>\n",
            "\n",
            "O que disse pra faz que já praisente? É sola mai-vi! <|endofjoke|>\n",
            "\n",
            "Qual está sua estafa dos japonos? Númera de Aproxem! <|endofjoke|>\n",
            "\n",
            "Porque os asquereses de asis que não eu tá uma estafa... É carvalho! <|endofjoke|>\n",
            "\n",
            "Quem especialmente da terraça... Para seus eleu... <|endofjoke|>\n",
            "\n",
            "Qual a luzca está quando dessa a passa do Õnibus? Tiro-A-Jove <|endofjoke|>\n",
            "\n",
            "Qual a segunda que diz e a ruta pra se calto? O Vaz-Vaz <|endofjoke|>\n",
            "\n",
            "O que é mata gosta com um sistema? Sainte <|endofjoke|>\n",
            "\n",
            "Empoca que tem código de comer as cada que só que tem as cada? <|endofjoke|>\n",
            "\n",
            "Dixiela que é que se o carro das seus escalarándico? Eu vinga eu gostava. <|endofjoke|>\n",
            "\n",
            "Por que os pessoas com o nome do carro? Por que Ele aquele aquele que ele um carro de carro. <|endofjoke|>\n",
            "\n",
            "Vive que estor quando uma só perguntiva em comunicado? Você não gêdas não entrou eles, até açou ela é cada um cidade açucão <|endofjoke|>\n",
            "\n",
            "Qual o jogo e que o N-star que a joveme o não o que acontece? A vinta. <|endofjoke|>\n",
            "\n",
            "Compulsou o meixico um sessores difícil? Bifidão que açúcar. <|endofjoke|>\n",
            "\n",
            "Qual jornal de tomates mais de cantatos do mundo? No, a Úlamas como o vírus <|endofjoke|>\n",
            "\n",
            "Como se churrle é a cidade come dá seria? Quatro no mãe de uma uma eu, vc? Ela do um, uma eu eu, uma eu eeu. <|endofjoke|>\n",
            "\n",
            "Qual o corretor que eu eu seus faltas? Estado, eu seus nascimento. <|endofjoke|>\n",
            "\n",
            "Quem você vai que uma nome para até ainda quando uma rios de venceu? Um cara. <|endofjoke|>\n",
            "\n",
            "Você elês uma seu estilo, sempre estove a seu ou ouço uma pega o dia ele não é? O seus-e-meu. <|endofjoke|>\n",
            "\n",
            "Todos mais cama as menos e suam mãe do Marou e meu oucem. São ocuni <|endofjoke|>\n",
            "\n",
            "Você sabe esse perto soga do mundo... O jogo que o único. <|endofjoke|>\n",
            "\n",
            "Fui aquele gosta. Qual diferença não atingido? Ele queria, avelia. <|endofjoke|>\n",
            "\n",
            "Borra sabe a vinaiga, porque foi fazendo. Porque ele é o é não dá fica a melhor! <|endofjoke|>\n",
            "\n",
            "Se o crava o que o vê não se vi? Que pada? <|endofjoke|>\n",
            "\n",
            "Quem sempre fazer ou uma teia? Quando se umas não seus nôles <|endofjoke|>\n",
            "\n",
            "Por que o jogo vender um perto? Qual o jogo? Para estão de após <|endofjoke|>\n",
            "\n",
            "Qual mas mácesses que esperanhas em uma\n",
            "\n",
            "[410 | 1758.52] loss=2.78 avg=3.22\n",
            "[420 | 1800.39] loss=2.92 avg=3.22\n",
            "[430 | 1842.30] loss=2.93 avg=3.21\n",
            "[440 | 1884.20] loss=2.89 avg=3.20\n",
            "[450 | 1926.13] loss=2.36 avg=3.18\n",
            "[460 | 1968.05] loss=2.90 avg=3.17\n",
            "[470 | 2009.98] loss=2.81 avg=3.16\n",
            "[480 | 2051.95] loss=2.91 avg=3.15\n",
            "[490 | 2093.86] loss=2.85 avg=3.14\n",
            "[500 | 2135.84] loss=2.72 avg=3.13\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 2180.93] loss=2.87 avg=3.13\n",
            "[520 | 2222.83] loss=2.73 avg=3.12\n",
            "[530 | 2264.61] loss=2.74 avg=3.11\n",
            "[540 | 2306.36] loss=2.68 avg=3.10\n",
            "[550 | 2348.06] loss=2.84 avg=3.09\n",
            "[560 | 2389.79] loss=2.65 avg=3.08\n",
            "[570 | 2431.47] loss=2.80 avg=3.07\n",
            "[580 | 2473.17] loss=2.78 avg=3.07\n",
            "[590 | 2514.89] loss=2.73 avg=3.06\n",
            "[600 | 2556.65] loss=2.80 avg=3.05\n",
            "======== SAMPLE 1 ========\n",
            " engendido, o nome de um amigo para mídio? O, na última. <|endofjoke|>\n",
            "O que o ouro mais tio do seu mãe? O, um é o nome é o que mãe de tiver se engenhei? Engenhei de tão. <|endofjoke|>\n",
            "A cidade ficou a chamada O que vai não então do chamada. <|endofjoke|>\n",
            "Fui, sabia da chorar com uma carro de um sistema que ele nanchó a mãe, achá de vai de vem sei o açutal. Qual o nome do filme? A céu de biscofo. <|endofjoke|>\n",
            "Um cachorro estraram um aúdo... Tiveles! <|endofjoke|>\n",
            "O número câmara de água que você gostava? Qual o ministro do casa que se a casa de casa ? <|endofjoke|>\n",
            "Qual a música mais ao doença? A último O César! <|endofjoke|>\n",
            "Por que o síndico mais tanto o pão de área? Por que ele é um eu de ocasiado. <|endofjoke|>\n",
            "Qual o tão perto a corre? A lite. <|endofjoke|>\n",
            "Céu vô faz o pão... Éu faz o pão de fazer. <|endofjoke|>\n",
            "Euvir se acidente é ciudad delaço, que cinco as fórmas os piscivos muitas comunismo poru, é a sua têm ou ciudade. É o cinco de filósofo... <|endofjoke|>\n",
            "Quem é o cinco as a pessoas? O galinhão <|endofjoke|>\n",
            "Você é o uma otorrido do meu doje uma otorrido do usado. Qual diferença a língua? Tinto. <|endofjoke|>\n",
            "O que aconteceu quando não era pá? ... eu acho, o último um aula e ele fazer: a tinto... <|endofjoke|>\n",
            "Por que a casa que sei o um fazer chamado? Porque ele vai é a casa, não ele tem a casa de fómente. <|endofjoke|>\n",
            "Qual o nome do fórmulas que quando não pode sai quando não seguro? Porque ele são foi ao jamaçonel. <|endofjoke|>\n",
            "Qual o veículo que não vai com o que uma minha? O pão de ficar-choco. <|endofjoke|>\n",
            "Qual o pão de corre o músico mais suário? O Ticuyinho <|endofjoke|>\n",
            "Qual o nome daquela ao pessoa que ele frio da pão do mundo? O a Bães-mí-tão <|endofjoke|>\n",
            "Já contar o cara do público que ele um poca do minora no añário. Não já faz, tá sempre querer muito vam a pão de pê <|endofjoke|>\n",
            "O que o fala foi fazer? Foi, fei fei <|endofjoke|>\n",
            "O que é um japona gosta não crija foi? eu gosta, é um japona gosta. <|endofjoke|>\n",
            "Cera sempre um corre o outro, seu fala de têm na cerveja. Aquele quem não é o outro nel e não falta, não é o crija? <|endofjoke|>\n",
            "O que aconteceu para o criança do cara que está em bate de casa? Porque ela estava a semel\n",
            "\n",
            "[610 | 2616.47] loss=2.69 avg=3.05\n",
            "[620 | 2658.17] loss=2.70 avg=3.04\n",
            "[630 | 2699.94] loss=2.72 avg=3.03\n",
            "[640 | 2741.69] loss=2.69 avg=3.03\n",
            "[650 | 2783.43] loss=2.53 avg=3.02\n",
            "[660 | 2825.16] loss=2.67 avg=3.01\n",
            "[670 | 2866.83] loss=2.54 avg=3.00\n",
            "[680 | 2908.58] loss=2.58 avg=2.99\n",
            "[690 | 2950.28] loss=2.68 avg=2.98\n",
            "[700 | 2991.95] loss=2.52 avg=2.97\n",
            "[710 | 3033.66] loss=2.70 avg=2.97\n",
            "[720 | 3075.34] loss=2.65 avg=2.96\n",
            "[730 | 3117.06] loss=2.63 avg=2.96\n",
            "[740 | 3158.76] loss=2.40 avg=2.95\n",
            "[750 | 3200.48] loss=2.46 avg=2.94\n",
            "[760 | 3242.20] loss=2.52 avg=2.93\n",
            "[770 | 3283.93] loss=2.65 avg=2.92\n",
            "[780 | 3325.70] loss=2.38 avg=2.91\n",
            "[790 | 3367.48] loss=2.47 avg=2.91\n",
            "[800 | 3409.25] loss=2.44 avg=2.90\n",
            "======== SAMPLE 1 ========\n",
            "jos a atirante? O que vender das écremotes. <|endofjoke|> \n",
            "Um queijor de cagemante anos do férias pra outro fábrica... É o filha. <|endofjoke|> \n",
            "Por que a mulher de casa da vila na cama? Nada. <|endofjoke|> \n",
            "Por que a mulher de pico e ani a o ponto-lá pra embarada? P E A P R <|endofjoke|> \n",
            "Vocês sabem quatro jogaam ao vender? O homem vampir, que uma évidigo. <|endofjoke|> \n",
            "Aos 14 meio sabenho pra se de cabeçao... Quem é da sabeste de nacha? <|endofjoke|> \n",
            "Porque a roupa não mais de vídeo ser peixe? Porque eles a pessoa da Mãe. <|endofjoke|> \n",
            "Alguém a área de fofava de chega de criança o outro foIeu a mãe de vida? FoIeu, a-mão o daquele-mãe. <|endofjoke|> \n",
            "Qual é a maior fórem do tempo do tempo do tempo da água?  O tempo do tempo do tempo do tempo do tempo... <|endofjoke|> \n",
            "Por que o cumbília é fica no última música? Porque é estão múses <|endofjoke|> \n",
            "Qual é a piada da água? A piada do filho. <|endofjoke|> \n",
            "Minha temo um prática oqueemia do mundo. Um dia de garrulhas. O nacha da último de jacarão. <|endofjoke|> \n",
            "Qual é a bola de saúde o deve A trifidro para o da quita dos escolações? A trifidro de saúde o comida: \"Para no dia de saúce\". <|endofjoke|> \n",
            "Quais vegas chamam um mórdica o número preferido daqui? A trifidro da estrelação <|endofjoke|> \n",
            "Qual empresa do mundo que os nós seguras? É um nena, no uma número <|endofjoke|> \n",
            "Qual entre dos nós tem as pessoas e pessoas não acusa? A mãe, eles não tem <|endofjoke|> \n",
            "Por que ano pra mais triste para quê? Porque ela estão dizer a cabeça <|endofjoke|> \n",
            "Qual é a foi que triste dos chamados? Foi aí. <|endofjoke|> \n",
            "Por que o bábex fosse pode pro nome não dizem? Porque ele manda não tem. <|endofjoke|> \n",
            "A jornada dar o óual de trabalha que o que a fórmula sozinho. Qual o nome do filme? A Bábex triste <|endofjoke|> \n",
            "Por que no bábex fica de triste? Porque não falam eles fez amas. <|endofjoke|> \n",
            "O que aconteceu quando elefantes cerveja sambuches foi encontro? Ele ficou porque estava pia de parecs, não sei de um ficar de pisa. <|endofjoke|> \n",
            "O que é um faz de ser um aplicada de Æcade? A dógada e seu comum. <|endofjoke|> \n",
            "Qual é o estilo que toma um pessoa amazoninha? A mãe pessoa, eles nunca só pessas de sartor. <|endofjoke|> \n",
            "Qual o carro que só faz tô? É o cólogo\n",
            "\n",
            "[810 | 3469.23] loss=2.49 avg=2.89\n",
            "[820 | 3511.00] loss=2.50 avg=2.88\n",
            "[830 | 3552.78] loss=2.41 avg=2.87\n",
            "[840 | 3594.56] loss=2.44 avg=2.87\n",
            "[850 | 3636.30] loss=2.25 avg=2.86\n",
            "[860 | 3678.04] loss=2.56 avg=2.85\n",
            "[870 | 3719.81] loss=2.32 avg=2.84\n",
            "[880 | 3761.58] loss=2.30 avg=2.83\n",
            "[890 | 3803.33] loss=2.48 avg=2.83\n",
            "[900 | 3845.07] loss=2.53 avg=2.82\n",
            "[910 | 3886.80] loss=2.65 avg=2.82\n",
            "[920 | 3928.50] loss=2.41 avg=2.81\n",
            "[930 | 3970.20] loss=2.52 avg=2.81\n",
            "[940 | 4011.95] loss=2.38 avg=2.80\n",
            "[950 | 4053.64] loss=2.48 avg=2.80\n",
            "[960 | 4095.41] loss=2.40 avg=2.79\n",
            "[970 | 4137.23] loss=1.55 avg=2.77\n",
            "[980 | 4179.06] loss=2.37 avg=2.76\n",
            "[990 | 4220.94] loss=2.32 avg=2.76\n",
            "[1000 | 4262.82] loss=2.26 avg=2.75\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "Cherie! <|endofjoke|>\n",
            "O que aconteceu com um amigo conhecido? Um jovial! <|endofjoke|>\n",
            "O que acontece convidiu se um cachorro com fazer? Cachaça-fra-lada. <|endofjoke|>\n",
            "Por que as amigas não foi ganhar de você quando ele é você do que no míquilo? Porque ele deixa o você não me ainda. <|endofjoke|>\n",
            "Por que a água vai no meio do pavê? Pq é um arê. <|endofjoke|>\n",
            "Paiso para o padre dois japonês? <|endofjoke|>\n",
            "O que é um jogado? É um russa. <|endofjoke|>\n",
            "O que é um jogado? É um rofa. <|endofjoke|>\n",
            "o que é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é é no puma o nome é? É nome. <|endofjoke|>\n",
            "Estaram um maconhece e como pessoas? Queria pessoas não conhece que, o Deus conhece em mais? <|endofjoke|>\n",
            "Qual é o animal mais que está sempre sem mamãos? Porque é a pessoa pra sem cumprão. <|endofjoke|>\n",
            "Os japoneses de física e não falam. <|endofjoke|>\n",
            "Por que o vampiro as pessoas, peixes? Porque o vampros na água. <|endofjoke|>\n",
            "O que é uma sambutà no série na igreja? É o César as mãos <|endofjoke|>\n",
            "Vcs gosta de fofilo que é o de um pés. Para pedia ele dizer um e-verde. <|endofjoke|>\n",
            "Qual o homem que gosta de fofo? O fofo-queleo. <|endofjoke|>\n",
            "O que é o que é não se chama pra ser moto? \"A\" \"lá-cidade\" \"São-lá-la\" \"la\". <|endofjoke|>\n",
            "Pq a liga várias tá cagarada? Pq uí-raía <|endofjoke|>\n",
            "Qual o nome da cria que todo mundo visto? Urdin Tóusko. <|endofjoke|>\n",
            "Como se eu nome a cerveja da tarda? Ele má cerveja da tarda. <|endofjoke|>\n",
            "O que é uma minha pessoa? É uma minha tá fase <|endofjoke|>\n",
            "Se O Tio do Thor o Thor que ficou doer que fazer... ...este uma bico... <|endofjoke|>\n",
            "O que é um caminhão? É amarelo. <|endofjoke|>\n",
            "Qual a semelhança entre o nome para a melhor na tia... ... na cabeça não faz. <|endofjoke|>\n",
            "Por que o índio pela fazer tão nadar? Porque o tio do gé a ó-rite. <|endofjoke|>\n",
            "Por que o melhor é um jogo? Porque já acontecer o médico <|endofjoke|>\n",
            "Você sabe porque ele é chove? <|endofjoke|>\n",
            "O que é um quisquier? <|endofjoke|>\n",
            "O que é um quisquier? É que é só bom? 🤷🤁 <|endofjoke|>\n",
            "Por que não é o ativoso? Porque eles gêr-pau-kau <|endofjoke|>\n",
            "Por que é um caminhão? Por que\n",
            "\n",
            "[1010 | 4325.57] loss=2.39 avg=2.74\n",
            "[1020 | 4367.41] loss=2.25 avg=2.73\n",
            "[1030 | 4409.27] loss=2.42 avg=2.73\n",
            "[1040 | 4451.08] loss=2.24 avg=2.72\n",
            "[1050 | 4492.92] loss=2.42 avg=2.72\n",
            "[1060 | 4534.75] loss=2.34 avg=2.71\n",
            "[1070 | 4576.61] loss=2.45 avg=2.71\n",
            "[1080 | 4618.46] loss=2.37 avg=2.70\n",
            "[1090 | 4660.30] loss=2.24 avg=2.70\n",
            "[1100 | 4702.14] loss=2.05 avg=2.69\n",
            "[1110 | 4743.97] loss=2.35 avg=2.68\n",
            "[1120 | 4785.79] loss=2.20 avg=2.67\n",
            "[1130 | 4827.63] loss=2.33 avg=2.67\n",
            "[1140 | 4869.43] loss=2.27 avg=2.66\n",
            "[1150 | 4911.25] loss=2.14 avg=2.66\n",
            "[1160 | 4953.08] loss=2.51 avg=2.65\n",
            "[1170 | 4994.91] loss=2.13 avg=2.65\n",
            "[1180 | 5036.74] loss=2.12 avg=2.64\n",
            "[1190 | 5078.59] loss=2.06 avg=2.63\n",
            "[1200 | 5120.42] loss=2.17 avg=2.62\n",
            "======== SAMPLE 1 ========\n",
            "nd> !!! A álcerne no meu pequeno? <|endofjoke|>\n",
            "Se a sugua contato está em lugar no meio da Vela? O Poito no meio. <|endofjoke|>\n",
            "Qual é o médico preferido dos vírdias? Ralhã <|endofjoke|>\n",
            "Qual a doença favorita da rex? R. Vírus <|endofjoke|>\n",
            "Quem é o relacionamento entre o dia vão? O Sextu <|endofjoke|>\n",
            "O que a sucesso disse para 1 ? <|endofjoke|>\n",
            "Criaram àsles a loça de vó tava fotografia... Quando ele queria acabou. <|endofjoke|>\n",
            "Por que a sucesso disse que ficante disse? Pra se formar comidação do mundo... <|endofjoke|>\n",
            "Você q vc é um caixa que ele é um dado? Com um caixa que é um dado? <|endofjoke|>\n",
            "Minha mãe é o que ser amarelo? Aima, vamos lá tem <|endofjoke|>\n",
            "Por que o Batman não usa super-heroes? Porque ele tem mulheres da Batman. <|endofjoke|>\n",
            "Um sabonete com um cachorro quando lá... O Capedan <|endofjoke|>\n",
            "Já vi um pobhito... ...vo lá, quando vai paga pau, seus apressam deixar um pobhe, vai o cachorro lá. <|endofjoke|>\n",
            "Não fazem de um gordo, sempre deixar um pobre... ...porque quando a mêsica não sua cadeira. <|endofjoke|>\n",
            "Por que o aluno perdo joga não consegue chamados... que quando qu'il é dois gatos? Porque a segunda não consegue <|endofjoke|>\n",
            "Qual jogador de game que era muito grande bem? O Pokémon. <|endofjoke|>\n",
            "Crieu vc ficava quando chove, muito filhos. <|endofjoke|>\n",
            "Só chama um jukebox. Quando esperando <|endofjoke|>\n",
            "Por que os brasileiro chamado O que os chamados <|endofjoke|>\n",
            "Porque não conseguiu nunca apenas, pode até o primeiro série? Porque fica de prisionaram. <|endofjoke|>\n",
            "O que é que a água se pode faz na bola? Ele canta na frente. <|endofjoke|>\n",
            "O que é que o que é? Um caixa... <|endofjoke|>\n",
            "Por que os brasileiro de Ucadinha Não conseguiu em são bons dá? <|endofjoke|>\n",
            "Aquele perna que sempre se encontrar a sua vida Eu estava o nome. <|endofjoke|>\n",
            "Sabe por que pessoas pessostas roubais? Porque perdeu um otto reosa, mas ladei em comunica. <|endofjoke|>\n",
            "Vamos não consegue jogar lá pode ser deixar de pessoas... ai muitos pediu no site um óeslav Klô. <|endofjoke|>\n",
            "O que o Batman faz querem juntos? Batman Batman! <|endofjoke|>\n",
            "Aquelas tios estão as comunicais com uma nacional de recuperação, mas não entende também <|endofjoke|>\n",
            "Eu sei em cima de arquitões no vó bibada, com uma matemática Mas não consegue cima de arquitões, com uma matemática <|endofjoke|>\n",
            "O que tem 5 deuzâ\n",
            "\n",
            "[1210 | 5180.73] loss=2.20 avg=2.62\n",
            "[1220 | 5222.54] loss=1.99 avg=2.61\n",
            "[1230 | 5264.39] loss=2.16 avg=2.60\n",
            "[1240 | 5306.23] loss=2.22 avg=2.60\n",
            "[1250 | 5348.05] loss=2.15 avg=2.59\n",
            "[1260 | 5389.91] loss=2.04 avg=2.58\n",
            "[1270 | 5431.79] loss=2.13 avg=2.58\n",
            "[1280 | 5473.63] loss=2.07 avg=2.57\n",
            "[1290 | 5515.48] loss=1.71 avg=2.56\n",
            "[1300 | 5557.34] loss=1.69 avg=2.55\n",
            "[1310 | 5599.19] loss=2.13 avg=2.54\n",
            "[1320 | 5641.07] loss=2.19 avg=2.54\n",
            "[1330 | 5682.92] loss=2.25 avg=2.53\n",
            "[1340 | 5724.74] loss=2.01 avg=2.52\n",
            "[1350 | 5766.55] loss=1.99 avg=2.52\n",
            "[1360 | 5808.38] loss=2.10 avg=2.51\n",
            "[1370 | 5850.22] loss=2.16 avg=2.51\n",
            "[1380 | 5892.05] loss=2.18 avg=2.50\n",
            "[1390 | 5933.90] loss=1.98 avg=2.50\n",
            "[1400 | 5975.77] loss=1.54 avg=2.48\n",
            "======== SAMPLE 1 ========\n",
            " pio! <|endofjoke|>\n",
            "Sabe por que a vila no pote com pedenas? Porque ela tem muita pintista <|endofjoke|>\n",
            "Já deixa com o espanhol otricado, qual o filme de música? O filme de nascimento <|endofjoke|>\n",
            "Por que se atravessando em cima com uma piscina? Porque ela quero a piscineja <|endofjoke|>\n",
            "Qual o jornal da banda que chamava das pernas? âia <|endofjoke|>\n",
            "Qual é o tipo de novo e español ? O tipo-da! <|endofjoke|>\n",
            "Porque o aplicativo giganza nova cria em falta? Porque ele réptil de vírus <|endofjoke|>\n",
            "Um arroz numa cachorro... Não como a corredor <|endofjoke|>\n",
            "Uma mulher ova minha brincar de um povo e pouco da minha fanta... Ele é um quase ficou minha fanta e só lheiopo <|endofjoke|>\n",
            "Qual o nome da pessoa que as fricas matamos? Ólhalo-pope <|endofjoke|>\n",
            "Caltarei para se for nov pra cumprirá pessoas... Então viu um brimjo e ficou brilhante <|endofjoke|>\n",
            "Aulas é um óculos pessoas e ficôs é fofa cagarço, matemátu não sei e seguro paulha <|endofjoke|>\n",
            "Um prócracq uma anão geralta e você só chinescent e eu estou tomate de ânus Semão e cça as minhas pessoas. E uma camaçúcar, vai sempre lá à noite <|endofjoke|>\n",
            "Quem é a pessoa que é o pão de jorquefô? O pita-joga <|endofjoke|>\n",
            "Qual carro ntoeste é o carro de pôneiros? Carro é dia <|endofjoke|>\n",
            "Qual é a cantora que é cantado? Ford F-back <|endofjoke|>\n",
            "A pessoa é o carro de gordho dá não sobre. A tequila faz Não pode ter a pessoa não gosta <|endofjoke|>\n",
            "Qual o queijo repetitivo para os menúdo brasileis? O que já jango <|endofjoke|>\n",
            "Aquelas dor muitos são paus. Tango são paltá-desistas, deveria o queijo pode vê <|endofjoke|>\n",
            "O que a ação disse para a outra? e o que já tem poura? Ele não ainda está bate e vê <|endofjoke|>\n",
            "Por que você fica com minha gato de três? Porque eu oo gato de gato. <|endofjoke|>\n",
            "Qual o filósofo que você gordho que o homem sem comunicar com fome de você?  O filósofo <|endofjoke|>\n",
            "Qual é o filósofo que você pode ser maconha? O filósofaulto <|endofjoke|>\n",
            "Uma mulher do escada vistoos que não têm ajudaram. Uma maram nunca tê muito do corredor <|endofjoke|>\n",
            "Que sobrinho que não tem um carro? Podou umas minhas. <|endofjoke|>\n",
            "Qual o problema de escolher que gosta de uma cidade entre pólenhos e um só corona? O páladó 23. <|endofjoke|>\n",
            "Se eu não falar de comentar o nuevo barbeu entra um tigre. Quer? Entra?\n",
            "\n",
            "[1410 | 6035.97] loss=2.08 avg=2.48\n",
            "[1420 | 6077.77] loss=1.92 avg=2.47\n",
            "[1430 | 6119.60] loss=1.98 avg=2.46\n",
            "[1440 | 6161.39] loss=2.00 avg=2.46\n",
            "[1450 | 6203.18] loss=2.08 avg=2.45\n",
            "[1460 | 6245.02] loss=1.97 avg=2.45\n",
            "[1470 | 6286.84] loss=1.99 avg=2.44\n",
            "[1480 | 6328.62] loss=2.02 avg=2.44\n",
            "[1490 | 6370.43] loss=2.19 avg=2.43\n",
            "[1500 | 6412.25] loss=1.94 avg=2.43\n",
            "Saving checkpoint/run1/model-1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5fe473-cd0f-4282-bb3f-42becf82bf97"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-1500\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf04c24-3a28-4bf4-d424-954af6df3e94"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aremen, November 3 (CNA) - A Colombian man was sentenced to 10-20 years in prison for evading an EU arrest warrant.\n",
            "\n",
            "Carlos Boulos, 40, of Amedeiros, was sent to a Maracay for 5.5 years. Mina Manqueirão, a social worker at Amedeiros hospital, said Boulos' crime was \"no crime at all\".\n",
            "\n",
            "\"No crime at all. No crime, no crime at all. I am a social worker, not a crime, no crime at all,\" Manqueirão said.\n",
            "\n",
            "READ MORE:\n",
            "\n",
            "* Evacueil de Água n'ava atração de fazer em ônibus\n",
            "\n",
            "* Evacueil de Água n'ava atração de só naquele líderada? Só computadorá, perguntaram de ação de contra-infóbienlandos e com todo mundo e manzar) (Por que uma biblioteca dá um fusca trontário)\n",
            "\n",
            "At a time when most crime in the continent is directed at the top, it is important to point out.\n",
            "\n",
            "\"No crime at all. No crime, no crime at all. I am a social worker, not a crime.\" -Carlos A. Boulos\n",
            "\n",
            "A remit of the interior and social protection and the interior justice so far is to decide on a term for Boulos' crime, and decide on a term for a crime at the interior, at the judicial and at the judicial and at the judicial and at the judicial and at the judicial and at the judicial and if not the judicial, at both for Tátulo, and for Yácalo, and at the judicial and therefor for Boulos. -iNEGRÁL RIETOS) (Amerões policia... Água ó avisou Ocado! Psicologica-porque-todos) (Por que o grupo já não quiser o tênima na rua) (Por que o corto quente não tênima na rua) (Por que o corto tênima na rua) (Por que o corto tênima na rio) (Por que o corto tênima na rino) (Por que o corto tênima na ronto) (Por que o corto tênima na ronto)<|endoftext|>So, it's time for us to decide on a Wonder Woman! Batavia is a portmanteua. <|endofjoke|\n",
            "Pssst, Batavia, let us be Wonder Women. <|endofjoke|\n",
            "\"Gone But Ever\" is a missive of the original.\n",
            "\"Grammy\" is a missive from \"The Rock\" <|endofjoke|\n",
            "\"Bom das fosse faz\" is a missive from \"The Batman\" <|endofjoke|\n",
            "Para crias é em mal\" is a missive from \"The Batman\" <|endofjoke|\n",
            "Para crias é é um bari novo? Para crias é é um bari e pqe que eu bari é mais não fazer na Batman e pqe que eu bari é é é pqe dizia. <|endofjoke|\n",
            "\"Dia dia, mas estando seu mão comentárico\" <|endofjoke|\n",
            "Porque toda a semel de nacional fumaba? Porque ela é um bêbadente. <|endofjoke|\n",
            "\"Por que o homem tem a alta de Thor?\" Porque ele é \"Thor\". <|endofjoke|\n",
            "Porque o pedreiro tem um tigre\" ...\"tina fica lunda\". <|endofjoke|\n",
            "Quando o último vem a prostituta, mas queria está difícil pra parar. <|endofjoke|\n",
            "Sabe porque o jacaré que não precisa a namorada? Porque ele se está no bom sistema. <|endofjoke|\n",
            "O que o brigade é terminador para vê? Só a brigade é falta. <|endofjoke|\n",
            "Por que o cachorro não quis amarelo? Porque tá sempre enquanto no caminho. <|end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "outputId": "a9738910-e34b-4c12-8c1b-33ee52b8ffe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=140,\n",
        "              temperature=0.8,\n",
        "              top_k=40,\n",
        "              prefix='Quando',\n",
        "              truncate='<|endofjoke|>',\n",
        "              nsamples=10,\n",
        "              batch_size=1,\n",
        "              )"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quando a pessoa atravessar com o pia ele! \n",
            "====================\n",
            "Quando para lá, uma coisa. \n",
            "====================\n",
            "Quando estava que ele se chama um dia. \n",
            "====================\n",
            "Quando estou e aparecida? A com falta \n",
            "====================\n",
            "Quando a dor em casa? O Corrinho \n",
            "====================\n",
            "Quando estava a meijo? Porque ela estava a lavender. \n",
            "====================\n",
            "Quando quisquando estava no meio do natal? Uma filha e ainda no tributa do céu, a comprar o nome. \n",
            "====================\n",
            "Quando, no churrasco no bar? O churrasco no céu! \n",
            "====================\n",
            "Quando é espirram? Na frita. \n",
            "====================\n",
            "Quando! \n",
            "====================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD"
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}